{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home My Daily Advanced K8s Study Notes Select the notes from the top Navigation","title":"Home"},{"location":"#home","text":"My Daily Advanced K8s Study Notes","title":"Home"},{"location":"Notes/01 Onwards to k8s/01 Why k8s/","text":"Why k8s Before we go further, we will try to find out the answers two most important question, What is k8s? Why we use k8s? Let's assume we a running multiple containers in the Elastic Beanstalk . If we started to get a lot of users to our application, we might need to scale the application. When we do the scaling, it is important to keep in mind, we only scale the container of the worker process, not all the container. But with Elastic Beanstalk this type of scaling will be real challenging. Let's find out how k8s can solve this entire scaling issue. In k8s there is k8s cluster that is made up with master + nodes . Here, nodes can be virtual machine or physical computer contains some number of different containers. And, master controls each of the nodes. So with a k8s cluster , we can have a node that will contain some containers these do not have to scale. And a bunch of other nodes contains the worker process container, they will be scaled according to the traffic. The master is running couple of programs to determine how the nodes will run all the containers on our behalf. As developer, we interact the k8s cluster only by the master . We send the instructions to the master and the master will pass all these instructions to the respective nodes . There will be a Load Balance in front of the k8s cluster . So the bottom line is, we can summarize the answer What is k8s? A container orchestrator make many servers act like one. Kubernetes is one of the most popular container orchestrator. Why we use k8s? Deploy the containerized apps.","title":"01 Why k8s"},{"location":"Notes/01 Onwards to k8s/01 Why k8s/#why-k8s","text":"Before we go further, we will try to find out the answers two most important question, What is k8s? Why we use k8s? Let's assume we a running multiple containers in the Elastic Beanstalk . If we started to get a lot of users to our application, we might need to scale the application. When we do the scaling, it is important to keep in mind, we only scale the container of the worker process, not all the container. But with Elastic Beanstalk this type of scaling will be real challenging. Let's find out how k8s can solve this entire scaling issue. In k8s there is k8s cluster that is made up with master + nodes . Here, nodes can be virtual machine or physical computer contains some number of different containers. And, master controls each of the nodes. So with a k8s cluster , we can have a node that will contain some containers these do not have to scale. And a bunch of other nodes contains the worker process container, they will be scaled according to the traffic. The master is running couple of programs to determine how the nodes will run all the containers on our behalf. As developer, we interact the k8s cluster only by the master . We send the instructions to the master and the master will pass all these instructions to the respective nodes . There will be a Load Balance in front of the k8s cluster . So the bottom line is, we can summarize the answer What is k8s? A container orchestrator make many servers act like one. Kubernetes is one of the most popular container orchestrator. Why we use k8s? Deploy the containerized apps.","title":"Why k8s"},{"location":"Notes/01 Onwards to k8s/02 k8s Tools/","text":"k8s Tools kubernetes : K8s is the short form of kubernetes . It's the whole orchestration system. kubectl : kubectl is abbreviated by kube control . It's the CLI for k8s and used to configure and manage the apps. Nodes : Single server/virtual machine in the k8s cluster. kubelet : An small agent, run in the Node to communicate between Node and the master . Control Plane : Also known as master . Control Panel or master is in the charge of whole cluster. It make sure, the current state of the cluster is matched with the desired state.","title":"02 k8s Tools"},{"location":"Notes/01 Onwards to k8s/02 k8s Tools/#k8s-tools","text":"kubernetes : K8s is the short form of kubernetes . It's the whole orchestration system. kubectl : kubectl is abbreviated by kube control . It's the CLI for k8s and used to configure and manage the apps. Nodes : Single server/virtual machine in the k8s cluster. kubelet : An small agent, run in the Node to communicate between Node and the master . Control Plane : Also known as master . Control Panel or master is in the charge of whole cluster. It make sure, the current state of the cluster is matched with the desired state.","title":"k8s Tools"},{"location":"Notes/01 Onwards to k8s/03 Local Environment for k8s/","text":"Local Environment for k8s To run k8s in the local environment, we will need two programs installed in the machine, Minikube kubectl Install Minikube Download the program, curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 To install, sudo install minikube-linux-amd64 /usr/local/bin/minikube This is optional. If the user is not added to the docker group , sudo usermod -aG docker $USER && newgrp docker Start Minikube by minikube start To check if minikube is running, check the status, minikube status Install kubectl Download the program, curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" To install sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl To test if the kubectl is installed, check the version, kubectl version","title":"03 Local Environment for k8s"},{"location":"Notes/01 Onwards to k8s/03 Local Environment for k8s/#local-environment-for-k8s","text":"To run k8s in the local environment, we will need two programs installed in the machine, Minikube kubectl Install Minikube Download the program, curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 To install, sudo install minikube-linux-amd64 /usr/local/bin/minikube This is optional. If the user is not added to the docker group , sudo usermod -aG docker $USER && newgrp docker Start Minikube by minikube start To check if minikube is running, check the status, minikube status Install kubectl Download the program, curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" To install sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl To test if the kubectl is installed, check the version, kubectl version","title":"Local Environment for k8s"},{"location":"Notes/01 Onwards to k8s/04 Verify Local Machine Setup/","text":"Verify Local Machine Setup We will run an docker image in the k8s cluster. Check the minikube status, minikube status We should see the status as running and configured. To test the cluster, kubectl cluster-info We should see the cluster is running. For any error/warning please check the installation section.","title":"04 Verify Local Machine Setup"},{"location":"Notes/01 Onwards to k8s/04 Verify Local Machine Setup/#verify-local-machine-setup","text":"We will run an docker image in the k8s cluster. Check the minikube status, minikube status We should see the status as running and configured. To test the cluster, kubectl cluster-info We should see the cluster is running. For any error/warning please check the installation section.","title":"Verify Local Machine Setup"},{"location":"Notes/01 Onwards to k8s/05 Object Types And API Versions/","text":"Object Types And API Versions In k8s world, apart from docker-compose , we need to consider, k8s expect all the containers image already be built In k8s cluster we have to manually do the networking Our client-pod.yml should be like the following, apiVersion: v1 kind: Pod metadata: name: proxy-pod labels: component: web spec: containers: - name: nginx-proxy image: nginx ports: - containerPort: 80 And our client-node-port.yml should be like the following, apiVersion: v1 kind: Service metadata: name: proxy-node-port spec: type: NodePort ports: - port: 3050 targetPort: 80 nodePort: 31516 selector: component: web In k8s world, when we are making a config file, we are not essentially making a container , instead we are making a object. These object/thing is being created inside the k8s engine to make the kind We made two configuration file, client-pod.yml and client-node-port.yml . We will feed these two configuration file to the kubectl . kubectl will take these two config file and create two objects out of them. In k8s there are couple types of objects, for example, StatefulSet ReplicaController Pod Service etc. These objects have different purposes, like, Running a container Monitoring Setting up networking etc. In our config file, we define the types of the object by the kind property. For example, in the client-pod.yml file, we set the object type Pod in the yml file by kind: Pod . similarly, in the client-node-port.yml file, we define the object type to Service by kind: Service . apiVersion Each apiVersion has a set of object types. For example, when the apiVersion is v1 , we can specify the objectTypes or kind as, componentStatus configMap Endpoints Event Namespace Pod PersistentVolumeClaim Secrets Instead, if the apiVersion is apps/v1 , we can use objectTypes or kind as, ControllerRevision StatefulSet Each API version has different set of objects type. So before we using any object types, we have to check the api Version and specify on the top of the yml file. Persistent Volume Access Modes In k8s there are 3 types of access modes for the persistent volumes, ReadWriteOnce : Only one node is allowed to do read/write operation in the volume. ReadOnlyMany : Multiple nodes can read from the volume at the same time. ReadWriteMany : Multiple nodes can perform both operation, read and write to the volume at the same time. Allocating Persistent Volume When we ask k8s for persistent volume, it reaches for some storage class. In local machine, the default storage class is a slice of the hard disk. This is a reason, we do not need to specify the storage class in local machine. We can see the list of storage class, kubectl get storageclass We can get details of the storage class by, kubectl describe storageclass When we run the k8s in cloud, then we have tons of options for storage class. For example, we can use AWS EBS , Google Cloud Persistence Disk , Azure File , Azure Disk etc. For each cloud provider, a default storage class is automatically configured. We can see the list of persistence volumes, kubectl get pv To see the persistent volume claims, kubectl get pvc This PVC list is showing, we can use these persistence volume. And the PV list is the actual use cases of these volumes.","title":"05 Object Types And API Versions"},{"location":"Notes/01 Onwards to k8s/05 Object Types And API Versions/#object-types-and-api-versions","text":"In k8s world, apart from docker-compose , we need to consider, k8s expect all the containers image already be built In k8s cluster we have to manually do the networking Our client-pod.yml should be like the following, apiVersion: v1 kind: Pod metadata: name: proxy-pod labels: component: web spec: containers: - name: nginx-proxy image: nginx ports: - containerPort: 80 And our client-node-port.yml should be like the following, apiVersion: v1 kind: Service metadata: name: proxy-node-port spec: type: NodePort ports: - port: 3050 targetPort: 80 nodePort: 31516 selector: component: web In k8s world, when we are making a config file, we are not essentially making a container , instead we are making a object. These object/thing is being created inside the k8s engine to make the kind We made two configuration file, client-pod.yml and client-node-port.yml . We will feed these two configuration file to the kubectl . kubectl will take these two config file and create two objects out of them. In k8s there are couple types of objects, for example, StatefulSet ReplicaController Pod Service etc. These objects have different purposes, like, Running a container Monitoring Setting up networking etc. In our config file, we define the types of the object by the kind property. For example, in the client-pod.yml file, we set the object type Pod in the yml file by kind: Pod . similarly, in the client-node-port.yml file, we define the object type to Service by kind: Service . apiVersion Each apiVersion has a set of object types. For example, when the apiVersion is v1 , we can specify the objectTypes or kind as, componentStatus configMap Endpoints Event Namespace Pod PersistentVolumeClaim Secrets Instead, if the apiVersion is apps/v1 , we can use objectTypes or kind as, ControllerRevision StatefulSet Each API version has different set of objects type. So before we using any object types, we have to check the api Version and specify on the top of the yml file.","title":"Object Types And API Versions"},{"location":"Notes/01 Onwards to k8s/05 Object Types And API Versions/#persistent-volume-access-modes","text":"In k8s there are 3 types of access modes for the persistent volumes, ReadWriteOnce : Only one node is allowed to do read/write operation in the volume. ReadOnlyMany : Multiple nodes can read from the volume at the same time. ReadWriteMany : Multiple nodes can perform both operation, read and write to the volume at the same time.","title":"Persistent Volume Access Modes"},{"location":"Notes/01 Onwards to k8s/05 Object Types And API Versions/#allocating-persistent-volume","text":"When we ask k8s for persistent volume, it reaches for some storage class. In local machine, the default storage class is a slice of the hard disk. This is a reason, we do not need to specify the storage class in local machine. We can see the list of storage class, kubectl get storageclass We can get details of the storage class by, kubectl describe storageclass When we run the k8s in cloud, then we have tons of options for storage class. For example, we can use AWS EBS , Google Cloud Persistence Disk , Azure File , Azure Disk etc. For each cloud provider, a default storage class is automatically configured. We can see the list of persistence volumes, kubectl get pv To see the persistent volume claims, kubectl get pvc This PVC list is showing, we can use these persistence volume. And the PV list is the actual use cases of these volumes.","title":"Allocating Persistent Volume"},{"location":"Notes/01 Onwards to k8s/06 Pod Object Types/","text":"Pod Object Types Whenever we start the minikube by minikube start , we started a virtual machine in the local machine. We call this virtual machine Node in terms of k8s . Pod is one of the most basic objects. Pod essentially is a grouping of containers of very similar purpose. Containers in a single Pod must be very tightly coupled and must be executed with each other. For example, if we have an application with API container and frontend container, we might not want to put them in a single Pod . But if there is a container for DB and another for Logging the DB , in this case both container should go inside a same Pod . In k8s we can not run a bare bone container without any overhead associate. A Pod is the minimum overhead to deploy a container in the k8s cluster. Inside Pod we have to run at least one or more container in it. When we push the Pod config file using kubectl to the k8s engine, it eventually create a Pod inside the Node aka Virtual Machine . We define which container will run inside the Pod , in the config file under spec -> containers .","title":"06 Pod Object Types"},{"location":"Notes/01 Onwards to k8s/06 Pod Object Types/#pod-object-types","text":"Whenever we start the minikube by minikube start , we started a virtual machine in the local machine. We call this virtual machine Node in terms of k8s . Pod is one of the most basic objects. Pod essentially is a grouping of containers of very similar purpose. Containers in a single Pod must be very tightly coupled and must be executed with each other. For example, if we have an application with API container and frontend container, we might not want to put them in a single Pod . But if there is a container for DB and another for Logging the DB , in this case both container should go inside a same Pod . In k8s we can not run a bare bone container without any overhead associate. A Pod is the minimum overhead to deploy a container in the k8s cluster. Inside Pod we have to run at least one or more container in it. When we push the Pod config file using kubectl to the k8s engine, it eventually create a Pod inside the Node aka Virtual Machine . We define which container will run inside the Pod , in the config file under spec -> containers .","title":"Pod Object Types"},{"location":"Notes/01 Onwards to k8s/07 Service Object Types/","text":"Service Object Types Service is all about setting up networking in the k8s cluster . Service types have 4 sub-types, NodePort ClusterIP LoadBalancer Ingress We define the sub-types under the spec property. NodePort NodePort is supposed to expose the container to the outside world. In most cases, we use NodePort inside the dev stage. There's a selector under the spec also. In the Service config file there is no reference of the Pod config file. In the Pod config file, we have a name and labels under the metadata . To send traffic to the Pod from the Service , we do not see any reference of the Pod . Instead in k8s , we use the label selector mechanism. In the Service file we have a selector under component: web , which is same as the Pod file labels, component: web . This is how the Service knows the Pod to send traffic. After selecting the Pod from the selector , comes the ports, under the spec -> NodePort -> ports we got 3 types of ports, ports : Internally other Pod / object connect through this port. targetPort : This should be same as the containerPort in the Pod config file. This is the open port of the container inside the Pod . nodePort : This is the port our application is exposed to the outside world. This port ranges 30000 to 32767. If we do not specify, it will generate a random port within this range. ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects ClusterIP It is a restrictive form of networking. It allows any objects inside the cluster to access the object it is pointed to. But outside ot the cluster, like from browser, it does not allow to access that object. Practically, ClusterIP allows anyone to access the object. Without this service, the object can not be accessed and inf we use NodePort service instead, it will expose the object to the outside world of the k8s cluster. It has 2 types of port, Port : In the k8s cluster , other objects can connect to the clusterIp service container using this port. targetPort : The ClientIP Service is pointing to the container with this targetPort . ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects Load Balancer This is an older way to handle traffic from outside to k8s cluster. With Load Balance Service , we replace the ClusterIp Service with it. And the k8s reach to the cloud provider to get there built in load balancer (For example, in AWS, it could be Classic Load Balancer or Application Load Balancer). When we use Load Balancer Service , it only allows the k8s cluster to expose only a set of specific Pods. The limitations are, it can not expose multiple set of pods. Ingress Service This Ingress service allows to come traffic inside the k8s cluster . When the traffic is inside the k8s cluster , effectively these traffic can also access the objects through ClusterIP` service.","title":"07 Service Object Types"},{"location":"Notes/01 Onwards to k8s/07 Service Object Types/#service-object-types","text":"Service is all about setting up networking in the k8s cluster . Service types have 4 sub-types, NodePort ClusterIP LoadBalancer Ingress We define the sub-types under the spec property. NodePort NodePort is supposed to expose the container to the outside world. In most cases, we use NodePort inside the dev stage. There's a selector under the spec also. In the Service config file there is no reference of the Pod config file. In the Pod config file, we have a name and labels under the metadata . To send traffic to the Pod from the Service , we do not see any reference of the Pod . Instead in k8s , we use the label selector mechanism. In the Service file we have a selector under component: web , which is same as the Pod file labels, component: web . This is how the Service knows the Pod to send traffic. After selecting the Pod from the selector , comes the ports, under the spec -> NodePort -> ports we got 3 types of ports, ports : Internally other Pod / object connect through this port. targetPort : This should be same as the containerPort in the Pod config file. This is the open port of the container inside the Pod . nodePort : This is the port our application is exposed to the outside world. This port ranges 30000 to 32767. If we do not specify, it will generate a random port within this range. ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects ClusterIP It is a restrictive form of networking. It allows any objects inside the cluster to access the object it is pointed to. But outside ot the cluster, like from browser, it does not allow to access that object. Practically, ClusterIP allows anyone to access the object. Without this service, the object can not be accessed and inf we use NodePort service instead, it will expose the object to the outside world of the k8s cluster. It has 2 types of port, Port : In the k8s cluster , other objects can connect to the clusterIp service container using this port. targetPort : The ClientIP Service is pointing to the container with this targetPort . ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects Load Balancer This is an older way to handle traffic from outside to k8s cluster. With Load Balance Service , we replace the ClusterIp Service with it. And the k8s reach to the cloud provider to get there built in load balancer (For example, in AWS, it could be Classic Load Balancer or Application Load Balancer). When we use Load Balancer Service , it only allows the k8s cluster to expose only a set of specific Pods. The limitations are, it can not expose multiple set of pods. Ingress Service This Ingress service allows to come traffic inside the k8s cluster . When the traffic is inside the k8s cluster , effectively these traffic can also access the objects through ClusterIP` service.","title":"Service Object Types"},{"location":"Notes/01 Onwards to k8s/08 Ingress Service/","text":"Ingress Service When it comes to ingress service in k8s, there are two options we have to consider, kubernetes-ingress : Maintained by the Nginx company itself. ingress-nginx : A community driven project. Comparatively, ingress nginx can be a better choice , we will use and discuss about the ingress-nginx here. Difference between them are discussed here . When it comes to set up ingress-nginx , it varies for different cloud provider. Controller : In k8s a controller is a kind of object, that constantly works to make the current state to the desired state. For example, when we create a Deployment Object using the config file, the Deployment Object observe the k8s cluster current state and ensure it to match the desired state. In a ingress service , we create a config file with some routing rules for the incoming traffic and send it off to the appropriate service inside the cluster. Now the kubectl create the ingress controller . Internally this ingress service is using Nginx . So this ingress controller will create a Pod with Nginx Controller in it. With this Nginx container the routes being handled and passed to different set of Pods . Ingress Nginx In Google Cloud As always, we have to create the config file of the ingress service . This config will be passed to a Deployment where the ingress controller and the Nginx Pod both will run. This Nginx Pod will take the incoming traffic and pass them to the appropriate set of Pods . In Google Cloud , when we create a ingress service , a load balancer Google Cloud Load Balancer will also be created for us. This Google Cloud Load Balancer is a cloud native service. When the Google Cloud Load Balancer is created outside of the cluster, a Load Balancer Service will also be created inside the k8s cluster . We know the Load Balancer Service is only capable of distribute traffic only a set of Pods . So here the Load Balancer Service will pass all these traffic to Nginx Pod and Nginx Pod will take care of the rest of traffic distribution between Pods . In dev, there is an additional deployment object, default-backend-pod , to ensure the health check of the other Pods . In production this will be replaced by the original application. Why ingress-nginx? We can simply make use of a Google Cloud Load Balancer , Load Balancer Service and Plain Nginx Pod . But we make use of the ingress-nginx because it is optimized and built for run in such k8s cluster environment. For example, when we are using Nginx Pod , it can bypass the Cluster Ip Service and pass traffic directly to the Pod . This feature is required for sticky session . Sticky Session enables the user to a client always communicate with the same server. Enabling Ingress To enable the ingress service in minikube , minikube addons enable ingress In output, we should see The 'ingress' addon is enabled . To verify, if the ingress service is enabled or not, kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch To detect, which version is being installed, nginx-ingress-controller --version","title":"08 Ingress Service"},{"location":"Notes/01 Onwards to k8s/08 Ingress Service/#ingress-service","text":"When it comes to ingress service in k8s, there are two options we have to consider, kubernetes-ingress : Maintained by the Nginx company itself. ingress-nginx : A community driven project. Comparatively, ingress nginx can be a better choice , we will use and discuss about the ingress-nginx here. Difference between them are discussed here . When it comes to set up ingress-nginx , it varies for different cloud provider. Controller : In k8s a controller is a kind of object, that constantly works to make the current state to the desired state. For example, when we create a Deployment Object using the config file, the Deployment Object observe the k8s cluster current state and ensure it to match the desired state. In a ingress service , we create a config file with some routing rules for the incoming traffic and send it off to the appropriate service inside the cluster. Now the kubectl create the ingress controller . Internally this ingress service is using Nginx . So this ingress controller will create a Pod with Nginx Controller in it. With this Nginx container the routes being handled and passed to different set of Pods . Ingress Nginx In Google Cloud As always, we have to create the config file of the ingress service . This config will be passed to a Deployment where the ingress controller and the Nginx Pod both will run. This Nginx Pod will take the incoming traffic and pass them to the appropriate set of Pods . In Google Cloud , when we create a ingress service , a load balancer Google Cloud Load Balancer will also be created for us. This Google Cloud Load Balancer is a cloud native service. When the Google Cloud Load Balancer is created outside of the cluster, a Load Balancer Service will also be created inside the k8s cluster . We know the Load Balancer Service is only capable of distribute traffic only a set of Pods . So here the Load Balancer Service will pass all these traffic to Nginx Pod and Nginx Pod will take care of the rest of traffic distribution between Pods . In dev, there is an additional deployment object, default-backend-pod , to ensure the health check of the other Pods . In production this will be replaced by the original application. Why ingress-nginx? We can simply make use of a Google Cloud Load Balancer , Load Balancer Service and Plain Nginx Pod . But we make use of the ingress-nginx because it is optimized and built for run in such k8s cluster environment. For example, when we are using Nginx Pod , it can bypass the Cluster Ip Service and pass traffic directly to the Pod . This feature is required for sticky session . Sticky Session enables the user to a client always communicate with the same server. Enabling Ingress To enable the ingress service in minikube , minikube addons enable ingress In output, we should see The 'ingress' addon is enabled . To verify, if the ingress service is enabled or not, kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch To detect, which version is being installed, nginx-ingress-controller --version","title":"Ingress Service"},{"location":"Notes/01 Onwards to k8s/09 Secret Object Types/","text":"Secret Object Types Since We should not write the secret in file/configuration, When we pass secret to the k8s it will be a imperative deployment. Secret itself is a object type. Instead of write a config file, we will use kubectl to create this secret object. 3 Types Of Secret generic : docker-registry : tls : When we pass --from-literal , it means we are passing the secret key and value in inline format. Another option can be load the secrets from a file, which is not ideal to load secrets. When we are using this Imperative Deployment to create secret, we have to execute the command for both in local environment and production cloud environment. To create a Secret object, we use, kubectl create secret <secret_type> <secret_name> --from-literal <key=value> This should give us output, secret/<secret_name> created . We can get the list of secrets, kubectl get secrets","title":"09 Secret Object Types"},{"location":"Notes/01 Onwards to k8s/09 Secret Object Types/#secret-object-types","text":"Since We should not write the secret in file/configuration, When we pass secret to the k8s it will be a imperative deployment. Secret itself is a object type. Instead of write a config file, we will use kubectl to create this secret object. 3 Types Of Secret generic : docker-registry : tls : When we pass --from-literal , it means we are passing the secret key and value in inline format. Another option can be load the secrets from a file, which is not ideal to load secrets. When we are using this Imperative Deployment to create secret, we have to execute the command for both in local environment and production cloud environment. To create a Secret object, we use, kubectl create secret <secret_type> <secret_name> --from-literal <key=value> This should give us output, secret/<secret_name> created . We can get the list of secrets, kubectl get secrets","title":"Secret Object Types"},{"location":"Notes/01 Onwards to k8s/10 Run Containers Inside k8s/","text":"Run Containers Inside k8s Now we will run a nginx server in the k8s cluster . A single container of nginx can not be an objective for k8s cluster , but for simplicity and see how thinks get work, we are doing the demo. Load the Pod config file, kubectl apply -f client-pod.yml We should see the output pod/proxy-pod created . Load the Service config file, kubectl apply -f client-node-port.yml We should see the output service/proxy-node-port created . We can get list of running pods, kubectl get pods We should see proxy-pod in running status. We can get the list of services, kubectl get services We should see proxy-node-port in the service list. All these services and ports are running not in the localhost instead, inside an IP provided by the minikube . We can see the provided IP by, minikube ip This should give us an IP of the k8s cluster . Now, we can browse, http://ip:31516 and see the nginx server is up and running.","title":"10 Run Containers Inside k8s"},{"location":"Notes/01 Onwards to k8s/10 Run Containers Inside k8s/#run-containers-inside-k8s","text":"Now we will run a nginx server in the k8s cluster . A single container of nginx can not be an objective for k8s cluster , but for simplicity and see how thinks get work, we are doing the demo. Load the Pod config file, kubectl apply -f client-pod.yml We should see the output pod/proxy-pod created . Load the Service config file, kubectl apply -f client-node-port.yml We should see the output service/proxy-node-port created . We can get list of running pods, kubectl get pods We should see proxy-pod in running status. We can get the list of services, kubectl get services We should see proxy-node-port in the service list. All these services and ports are running not in the localhost instead, inside an IP provided by the minikube . We can see the provided IP by, minikube ip This should give us an IP of the k8s cluster . Now, we can browse, http://ip:31516 and see the nginx server is up and running.","title":"Run Containers Inside k8s"},{"location":"Notes/01 Onwards to k8s/11 k8s Development Flow/","text":"k8s Development Flow In the k8s world, we pass the config files like Pod or Service to the master using the kubectl . The master has a responsibility to determine how many container is being running inside the pods in various nodes. We can define which nodes will run which container and how many. The master monitor all these containers inside the Pod of the Node . If some nodes fails to run or crash on runtime, the master will run another to keep the expectation.","title":"11 k8s Development Flow"},{"location":"Notes/01 Onwards to k8s/11 k8s Development Flow/#k8s-development-flow","text":"In the k8s world, we pass the config files like Pod or Service to the master using the kubectl . The master has a responsibility to determine how many container is being running inside the pods in various nodes. We can define which nodes will run which container and how many. The master monitor all these containers inside the Pod of the Node . If some nodes fails to run or crash on runtime, the master will run another to keep the expectation.","title":"k8s Development Flow"},{"location":"Notes/01 Onwards to k8s/12 Imperative vs Declarative Deployments/","text":"Imperative vs Declarative Deployments Whenever we need to update/interact with a k8s cluster, we update/change the desired state in a config file and pass it to master. We should never directly go to the nodes directly and and update the nodes/pods/container. The master is constantly working to meet the desired state. With Imperative Deployments , we have to provide specific instructions step by step to reach a objective. But when we go through the Declarative Deployments , we only define the final objective to the master and master will make it happen. Whenever it comes to update status it's always good practice to update the config file and feed it to the master and follow Declarative Deployments .","title":"12 Imperative vs Declarative Deployments"},{"location":"Notes/01 Onwards to k8s/12 Imperative vs Declarative Deployments/#imperative-vs-declarative-deployments","text":"Whenever we need to update/interact with a k8s cluster, we update/change the desired state in a config file and pass it to master. We should never directly go to the nodes directly and and update the nodes/pods/container. The master is constantly working to meet the desired state. With Imperative Deployments , we have to provide specific instructions step by step to reach a objective. But when we go through the Declarative Deployments , we only define the final objective to the master and master will make it happen. Whenever it comes to update status it's always good practice to update the config file and feed it to the master and follow Declarative Deployments .","title":"Imperative vs Declarative Deployments"},{"location":"Notes/01 Onwards to k8s/13 Clear Environment/","text":"Clear Environment To remove an object, we can take two approaches, Delete by object name, kubectl delete object_type object_name Delete by object file, kubectl delete -f config_file_name If we want to delete by file name, we have to pass the right file name and path in as config_file_name . Some examples of clearing the k8s objects are, Removing Pods Object (Delete by file name) We can remove an object from the k8s cluster by kubectl delete -f config_file_name . So the Pod , that is created from the config file client-pod.yml can be deleted by, kubectl delete -f client-pod.yml We can verify Pods being deleted by get the list of the Pods, kubectl get pods This is also an example of Imperative Deployment . Removing Deployments Object (Delete by object name) Get the list of deployments, kubectl get deployments This should give us all the running Deployment object name. We can delete the Deployment object by the name, kubectl delete deployment deployment_object_name As output, we should see deployment.apps \"deployment_object_name\" deleted . Removing Services Object (Delete by object name) Get the list of services, kubectl get services This should give us all the running Service object name. We can delete the Service object by the name, kubectl delete service service_object_name As output, we should see service.apps \"service_object_name\" deleted . In the service list, there should be a service object named kubernetes and it is internally used by kubernetes itself. We should not delete or mess with this service. To remove the whole cluster, minikube delete","title":"13 Clear Environment"},{"location":"Notes/01 Onwards to k8s/13 Clear Environment/#clear-environment","text":"To remove an object, we can take two approaches, Delete by object name, kubectl delete object_type object_name Delete by object file, kubectl delete -f config_file_name If we want to delete by file name, we have to pass the right file name and path in as config_file_name . Some examples of clearing the k8s objects are, Removing Pods Object (Delete by file name) We can remove an object from the k8s cluster by kubectl delete -f config_file_name . So the Pod , that is created from the config file client-pod.yml can be deleted by, kubectl delete -f client-pod.yml We can verify Pods being deleted by get the list of the Pods, kubectl get pods This is also an example of Imperative Deployment . Removing Deployments Object (Delete by object name) Get the list of deployments, kubectl get deployments This should give us all the running Deployment object name. We can delete the Deployment object by the name, kubectl delete deployment deployment_object_name As output, we should see deployment.apps \"deployment_object_name\" deleted . Removing Services Object (Delete by object name) Get the list of services, kubectl get services This should give us all the running Service object name. We can delete the Service object by the name, kubectl delete service service_object_name As output, we should see service.apps \"service_object_name\" deleted . In the service list, there should be a service object named kubernetes and it is internally used by kubernetes itself. We should not delete or mess with this service. To remove the whole cluster, minikube delete","title":"Clear Environment"},{"location":"Notes/01 Onwards to k8s/14 k8s in Production/","text":"k8s in Production When we choose to use k8s as orchestrator, then we need to choose the vendors for it. We can go either Cloud Managed Solution like GCP , AWS , Azure , Digital Ocean etc. We can also choose self managed distribution like, Docker Enterprise , Rancher , OpenShift , Canonical , VMWARE PKS etc.","title":"14 k8s in Production"},{"location":"Notes/01 Onwards to k8s/14 k8s in Production/#k8s-in-production","text":"When we choose to use k8s as orchestrator, then we need to choose the vendors for it. We can go either Cloud Managed Solution like GCP , AWS , Azure , Digital Ocean etc. We can also choose self managed distribution like, Docker Enterprise , Rancher , OpenShift , Canonical , VMWARE PKS etc.","title":"k8s in Production"},{"location":"Notes/02 Maintain Contianers/01 Update Existing Objects/","text":"Update Existing Objects When we need to update existing cluster state, we can either go imperative or declarative deployment. If we are running a cluster with a Pod object and Service object. In declarative deployment, when we need to update the cluster, we have to make sure the name and kind of the object should be same. We will write the updated state in the config file. With kubectl we will feed the master updated config file, and the master will do the rest. To do a hands on, first make sure we have a Pod and Service . The Pod config file client-pod.yaml is, apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 3000 Feed this Pod object to the master by, kubectl apply -f client-pod.yaml Amd the Service config file client-node-port.yaml is, apiVersion: v1 kind: Service metadata: name: client-node-port spec: type: NodePort ports: - port: 3050 targetPort: 3000 nodePort: 31515 selector: component: web Feed this Service object to master by, kubectl apply -f client-node-port.yaml","title":"01 Update Existing Objects"},{"location":"Notes/02 Maintain Contianers/01 Update Existing Objects/#update-existing-objects","text":"When we need to update existing cluster state, we can either go imperative or declarative deployment. If we are running a cluster with a Pod object and Service object. In declarative deployment, when we need to update the cluster, we have to make sure the name and kind of the object should be same. We will write the updated state in the config file. With kubectl we will feed the master updated config file, and the master will do the rest. To do a hands on, first make sure we have a Pod and Service . The Pod config file client-pod.yaml is, apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 3000 Feed this Pod object to the master by, kubectl apply -f client-pod.yaml Amd the Service config file client-node-port.yaml is, apiVersion: v1 kind: Service metadata: name: client-node-port spec: type: NodePort ports: - port: 3050 targetPort: 3000 nodePort: 31515 selector: component: web Feed this Service object to master by, kubectl apply -f client-node-port.yaml","title":"Update Existing Objects"},{"location":"Notes/02 Maintain Contianers/02 Declarative Update/","text":"Declarative Update Here we take an existing config file, leave its metadata->name and kind same, but update the image from stephengrider/multi-client to stephengrider/multi-worker . It's not our objective to run an entire application, we will just make sure, we can change the image of the pods. Updated the client-pod.yaml we deployed earlier with new image stephengrider/multi-worker , apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: stephengrider/multi-worker ports: - containerPort: 3000 Now feed the new Pod config file to master using kubectl by kubectl apply -f client-pod.yaml In output, we should see pod/client-pod configured . We can see all the pods list, kubectl get pods We should see the client-pod in the list. We can get details of the object info by, kubectl describe <object type> <object name> . In our case, we can look into client-pod details by, kubectl describe Pod client-pod In the details output, under Containers we should see Image: stephengrider/multi-worker . This is the exact image we used in the update config file. Also this can be considered as a declarative deployment.","title":"02 Declarative Update"},{"location":"Notes/02 Maintain Contianers/02 Declarative Update/#declarative-update","text":"Here we take an existing config file, leave its metadata->name and kind same, but update the image from stephengrider/multi-client to stephengrider/multi-worker . It's not our objective to run an entire application, we will just make sure, we can change the image of the pods. Updated the client-pod.yaml we deployed earlier with new image stephengrider/multi-worker , apiVersion: v1 kind: Pod metadata: name: client-pod labels: component: web spec: containers: - name: client image: stephengrider/multi-worker ports: - containerPort: 3000 Now feed the new Pod config file to master using kubectl by kubectl apply -f client-pod.yaml In output, we should see pod/client-pod configured . We can see all the pods list, kubectl get pods We should see the client-pod in the list. We can get details of the object info by, kubectl describe <object type> <object name> . In our case, we can look into client-pod details by, kubectl describe Pod client-pod In the details output, under Containers we should see Image: stephengrider/multi-worker . This is the exact image we used in the update config file. Also this can be considered as a declarative deployment.","title":"Declarative Update"},{"location":"Notes/02 Maintain Contianers/03 Limitations of Updating Config Files/","text":"Limitations of Updating Pods Config Files When we update the Pods config file for Declarative Deployments we can not update all the properties, only certain properties are there we are allowed to change like the image . For example, we can not update property like containers , name , port etc.","title":"03 Limitations of Updating Config Files"},{"location":"Notes/02 Maintain Contianers/03 Limitations of Updating Config Files/#limitations-of-updating-pods-config-files","text":"When we update the Pods config file for Declarative Deployments we can not update all the properties, only certain properties are there we are allowed to change like the image . For example, we can not update property like containers , name , port etc.","title":"Limitations of Updating Pods Config Files"},{"location":"Notes/02 Maintain Contianers/04 Deployment Object Types/","text":"Deployment Object Types Since, we have some limitations of using Pod for Declarative Deployment , we can make use of another types of object Deployment . Deployment object is meant to maintain a set of identical Pods. It keep the states of the Pod updated and ensure number of Pods . Pod Vs Deployment Both, Pod and Deployment are object types of k8s . Pods Run single set of tightly coupled containers Used in dev/local environment Deployment Runs set of identical Pods Monitor and keep the updated state of the Pods Good for dev and production environment When we create a Deployment object, we have to attach a Pod Template . A Pod Template is a block of Pod configuration. So essentially, every Pod we run with the Deployment object will follow the provided template configuration. Any time we change the config of the Pods Template , the Deployment first try to update the Pod configuration. If it fails to update Pod according to the updated Pods Template in this case, it will kill the existing Pod and create brand new Pod with updated template config.","title":"04 Deployment Object Types"},{"location":"Notes/02 Maintain Contianers/04 Deployment Object Types/#deployment-object-types","text":"Since, we have some limitations of using Pod for Declarative Deployment , we can make use of another types of object Deployment . Deployment object is meant to maintain a set of identical Pods. It keep the states of the Pod updated and ensure number of Pods . Pod Vs Deployment Both, Pod and Deployment are object types of k8s . Pods Run single set of tightly coupled containers Used in dev/local environment Deployment Runs set of identical Pods Monitor and keep the updated state of the Pods Good for dev and production environment When we create a Deployment object, we have to attach a Pod Template . A Pod Template is a block of Pod configuration. So essentially, every Pod we run with the Deployment object will follow the provided template configuration. Any time we change the config of the Pods Template , the Deployment first try to update the Pod configuration. If it fails to update Pod according to the updated Pods Template in this case, it will kill the existing Pod and create brand new Pod with updated template config.","title":"Deployment Object Types"},{"location":"Notes/02 Maintain Contianers/05 Deployment Config File/","text":"Deployment Config File Here, we create a config file for the Deployment object and feed the config file using kubectl to the master . Like previous, we are going to use a Pod Template that is responsible to run the multi-client app. Our deployment file client-deployment.yaml should be as follows, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 3000 apiVersion: We are using the Deployment object, that is defined in the apps/v1 api Kind : The object type is Deployment metadata -> name : This will be the created deployment object name client-deployment spec -> selector : With this selector, the Deployment object can handle the Pod . After a Pod is being create, the Pod also have a select-by-labels name. This should be same as the spec -> selector of the Deployment object. spec -> replicas : Number of pods will be created by the Deployment Object spec -> template : Config of the pods, will be used for every single pod we will create and maintain using the Deployment Object . This template is very much similar to the Pod config file.","title":"05 Deployment Config File"},{"location":"Notes/02 Maintain Contianers/05 Deployment Config File/#deployment-config-file","text":"Here, we create a config file for the Deployment object and feed the config file using kubectl to the master . Like previous, we are going to use a Pod Template that is responsible to run the multi-client app. Our deployment file client-deployment.yaml should be as follows, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 3000 apiVersion: We are using the Deployment object, that is defined in the apps/v1 api Kind : The object type is Deployment metadata -> name : This will be the created deployment object name client-deployment spec -> selector : With this selector, the Deployment object can handle the Pod . After a Pod is being create, the Pod also have a select-by-labels name. This should be same as the spec -> selector of the Deployment object. spec -> replicas : Number of pods will be created by the Deployment Object spec -> template : Config of the pods, will be used for every single pod we will create and maintain using the Deployment Object . This template is very much similar to the Pod config file.","title":"Deployment Config File"},{"location":"Notes/02 Maintain Contianers/06 Apply Deployment/","text":"Apply Deployment Before we deploy the client-deployment.yaml , make sure existing Pods are being deleted. We can verify no Pods is being running by kubectl get pods . Now we can deploy and create a Deployment Object by, kubectl apply -f client-deployment.yaml We should see an output eployment.apps/client-deployment created . If we get the list of running pods, kubectl get deployments We should see a Deployment object named client-deployment .","title":"06 Apply Deployment"},{"location":"Notes/02 Maintain Contianers/06 Apply Deployment/#apply-deployment","text":"Before we deploy the client-deployment.yaml , make sure existing Pods are being deleted. We can verify no Pods is being running by kubectl get pods . Now we can deploy and create a Deployment Object by, kubectl apply -f client-deployment.yaml We should see an output eployment.apps/client-deployment created . If we get the list of running pods, kubectl get deployments We should see a Deployment object named client-deployment .","title":"Apply Deployment"},{"location":"Notes/02 Maintain Contianers/07 Services Requirement/","text":"Requirement of Service Objects To access the container from browser using localhost. The minikube creates a virtual machine with IP to access these containers. We can get the ip of the virtual machine created by the minikube , minikube ip This should return the IP of the virtual machine. If we go to the http://virtual_machine_op:31515/ from browser, we should see the react app. kubectl get pods -o wide This will show the IP of the Pods. Every single Pods we have created always get an IP assigned. This IP address is internally assigned inside the Node and we can not directly access it from outside. If the Pod gets updated, changed or restarted, then the it is very much possible, the IP will be changed. So updating the IP in the development environment every time it is changed will be a big bottleneck. This is where we can make use of Service . With Service , the Service will be responsible to map traffic by the selector not the IP and we can get consistent address by the Service itself. So even though the Pods is being deleted, re-generated or restarted and getting a new IP, the Service object keeps these changes abstract to us.","title":"07 Services Requirement"},{"location":"Notes/02 Maintain Contianers/07 Services Requirement/#requirement-of-service-objects","text":"To access the container from browser using localhost. The minikube creates a virtual machine with IP to access these containers. We can get the ip of the virtual machine created by the minikube , minikube ip This should return the IP of the virtual machine. If we go to the http://virtual_machine_op:31515/ from browser, we should see the react app. kubectl get pods -o wide This will show the IP of the Pods. Every single Pods we have created always get an IP assigned. This IP address is internally assigned inside the Node and we can not directly access it from outside. If the Pod gets updated, changed or restarted, then the it is very much possible, the IP will be changed. So updating the IP in the development environment every time it is changed will be a big bottleneck. This is where we can make use of Service . With Service , the Service will be responsible to map traffic by the selector not the IP and we can get consistent address by the Service itself. So even though the Pods is being deleted, re-generated or restarted and getting a new IP, the Service object keeps these changes abstract to us.","title":"Requirement of Service Objects"},{"location":"Notes/02 Maintain Contianers/08 Updating Deployments/","text":"Updating Deployments Here we update the deployments config file and verify the associated Pods are updating accordingly. To do so, lets update the configuration file of the client-deployment.yaml with containerPort: 9999 . The updated client-deployment.yaml should be, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 9999 Now, we feed the updated config file to the master by the kubectl , kubectl apply -f client-deployment.yaml We should see output deployment.apps/client-deployment configured . This updated state essentially kill the existing Pod and create a new Pod with updated configuration. We can see the list of deployments object, kubectl get deployments We can see the updated/new Pod object by, kubectl get pods Since, with new configuration, we have to kill the exiting Pod and generate a new one, the pods age should be relatively very small. Now it is important to verify, the new Pod has the container port of 9999 . We can get pods details, kubectl describe pods object_name_created_by_client_deployment_config_file Under containers -> Client , we should see Port: 9999/TCP . So without doubt, the Pod is running with up to date configuration.","title":"08 Updating Deployments"},{"location":"Notes/02 Maintain Contianers/08 Updating Deployments/#updating-deployments","text":"Here we update the deployments config file and verify the associated Pods are updating accordingly. To do so, lets update the configuration file of the client-deployment.yaml with containerPort: 9999 . The updated client-deployment.yaml should be, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 9999 Now, we feed the updated config file to the master by the kubectl , kubectl apply -f client-deployment.yaml We should see output deployment.apps/client-deployment configured . This updated state essentially kill the existing Pod and create a new Pod with updated configuration. We can see the list of deployments object, kubectl get deployments We can see the updated/new Pod object by, kubectl get pods Since, with new configuration, we have to kill the exiting Pod and generate a new one, the pods age should be relatively very small. Now it is important to verify, the new Pod has the container port of 9999 . We can get pods details, kubectl describe pods object_name_created_by_client_deployment_config_file Under containers -> Client , we should see Port: 9999/TCP . So without doubt, the Pod is running with up to date configuration.","title":"Updating Deployments"},{"location":"Notes/02 Maintain Contianers/09 Scaling Deployments/","text":"Scaling Deployments If we have a requirement to use 5 Pods instead of 1 Pod , we can update the deployment config files replicas to 5 by replicas: 5 . Our scaled deployment config file, client-deployment.yaml should be following, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 5 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 9999 Now feed the config file to master using kubectl by, kubectl apply -f client-deployment.yaml We should get output deployment.apps/client-deployment configured . kubectl get deployments This should listed a deployment object with 5/5 , i.e. 5 pods are desired and 5 pods are running. Now if we look for the pods list, kubectl get pods We should see 5 Pods in running state, previously it was only 1 Pod.","title":"09 Scaling Deployments"},{"location":"Notes/02 Maintain Contianers/09 Scaling Deployments/#scaling-deployments","text":"If we have a requirement to use 5 Pods instead of 1 Pod , we can update the deployment config files replicas to 5 by replicas: 5 . Our scaled deployment config file, client-deployment.yaml should be following, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 5 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 9999 Now feed the config file to master using kubectl by, kubectl apply -f client-deployment.yaml We should get output deployment.apps/client-deployment configured . kubectl get deployments This should listed a deployment object with 5/5 , i.e. 5 pods are desired and 5 pods are running. Now if we look for the pods list, kubectl get pods We should see 5 Pods in running state, previously it was only 1 Pod.","title":"Scaling Deployments"},{"location":"Notes/02 Maintain Contianers/10 Updating Deployment Image/","text":"Updating Deployment Image In the world of k8s recreate the Pods for the updated version of image is quite complex. There's a popular issue in github on this topic. Traditionally we update/make change of the deployment file, we used to send the updated deployment file to the master using the kubectl command. In the deployment config file, there is no particular version of the image. So In case the image is updated in the docker hub, but the config file is not changed. kubectl does not change any changes in config and simply reject the file. k8s , on our behalf does not go and check if a new version of the image is available or not. This is why this is such a big issue. With this in our mind, we can consider 3 possible issues, Deleting Pod : We can simply delete the Pods. In this case, for the missing Pod, the master will create the Pods to match the desired state. So this time hopefully the latest image will be pulled in. We should get Pods with latest version of images. Image Tagging : We can tag the images while building and also specify the version in the deployment config file. Using Imperative Command : In this case, we will put image tag every time we build a new image. But instead of putting the image tag in the deployment config file, we will use imperative command to update the image version. None of these solutions are ideal. If we compare all these 3 options, we can definitely see the first one is very bad practice. Second one of Image Tagging is a pain for updating two config files every time. Among them using imperative command is comparatively much more usable. Here we will look into how can we update the deployment when a new version becomes available. We deploy the deployment with stephengrider/multi-client image, 3000 port and 1 replicas. Then we update the multi-client image and push the updated image to docker hub. Now ask the Deployment Object to recreate the Pods with the updated states. Our initial deployment config file client-deployment.yaml be like, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 3000 Feed this initial state to the k8s cluster by, kubectl apply -f client-deployment.yaml We should see output deployment.apps/client-deployment configured . To access the site, we first get the ip of the virtual machine, minikube ip This should output the IP of the virtual machine. Now browse http://virtual_machine_ip:31515/ we should see the react app is running. Now update the image. Let's say we are building new image with tag v5 . We can use imperative command to update the image inside the cluster using the format, kubectl set image <object_type> / <object_name> <container_name> = <new_image> In our case, we can interpret as, kubectl set image deployment/client-deployment client=stephengrider/multi-client:v5 Now in output, we should see deployment.apps/client-deployment image updated . We can see the pods, kubectl get pods We should notice the AGE of the pods is relatively small. This means without any doubt, the Pod is being recreated by the deployment. If we now browse http://virtual_machine_ip:31515/ we should see the react app is running and the title is changed to Fib Calculator version 2 This entire process is a little bit of pain. We have to rebuild the image Have to put a tag to the image Run an imperative command to update the image In production, there should be a script to handle these issues automatically.","title":"10 Updating Deployment Image"},{"location":"Notes/02 Maintain Contianers/10 Updating Deployment Image/#updating-deployment-image","text":"In the world of k8s recreate the Pods for the updated version of image is quite complex. There's a popular issue in github on this topic. Traditionally we update/make change of the deployment file, we used to send the updated deployment file to the master using the kubectl command. In the deployment config file, there is no particular version of the image. So In case the image is updated in the docker hub, but the config file is not changed. kubectl does not change any changes in config and simply reject the file. k8s , on our behalf does not go and check if a new version of the image is available or not. This is why this is such a big issue. With this in our mind, we can consider 3 possible issues, Deleting Pod : We can simply delete the Pods. In this case, for the missing Pod, the master will create the Pods to match the desired state. So this time hopefully the latest image will be pulled in. We should get Pods with latest version of images. Image Tagging : We can tag the images while building and also specify the version in the deployment config file. Using Imperative Command : In this case, we will put image tag every time we build a new image. But instead of putting the image tag in the deployment config file, we will use imperative command to update the image version. None of these solutions are ideal. If we compare all these 3 options, we can definitely see the first one is very bad practice. Second one of Image Tagging is a pain for updating two config files every time. Among them using imperative command is comparatively much more usable. Here we will look into how can we update the deployment when a new version becomes available. We deploy the deployment with stephengrider/multi-client image, 3000 port and 1 replicas. Then we update the multi-client image and push the updated image to docker hub. Now ask the Deployment Object to recreate the Pods with the updated states. Our initial deployment config file client-deployment.yaml be like, apiVersion: apps/v1 kind: Deployment metadata: name: client-deployment spec: replicas: 1 selector: matchLabels: component: web template: metadata: labels: component: web spec: containers: - name: client image: stephengrider/multi-client ports: - containerPort: 3000 Feed this initial state to the k8s cluster by, kubectl apply -f client-deployment.yaml We should see output deployment.apps/client-deployment configured . To access the site, we first get the ip of the virtual machine, minikube ip This should output the IP of the virtual machine. Now browse http://virtual_machine_ip:31515/ we should see the react app is running. Now update the image. Let's say we are building new image with tag v5 . We can use imperative command to update the image inside the cluster using the format, kubectl set image <object_type> / <object_name> <container_name> = <new_image> In our case, we can interpret as, kubectl set image deployment/client-deployment client=stephengrider/multi-client:v5 Now in output, we should see deployment.apps/client-deployment image updated . We can see the pods, kubectl get pods We should notice the AGE of the pods is relatively small. This means without any doubt, the Pod is being recreated by the deployment. If we now browse http://virtual_machine_ip:31515/ we should see the react app is running and the title is changed to Fib Calculator version 2 This entire process is a little bit of pain. We have to rebuild the image Have to put a tag to the image Run an imperative command to update the image In production, there should be a script to handle these issues automatically.","title":"Updating Deployment Image"},{"location":"Notes/02 Maintain Contianers/11 Multi Docker Access/","text":"Multi Docker Access We have two docker installed Inside local machine Inside the virtual machine, created by minikube By default our docker-client access the local docker-server . If we want to access the docker-server of the virtual machine , created by minikube , we can run, eval $(minikube docker-env) From this terminal, we can access the docker-server of the virtual machine. This is not a permanent re-configuration. This eval $(minikube docker-env) is a temporary connection of the virtual machine docker from the current terminal window. As soon as we go back to old terminal or open a new terminal, these terminal will make interaction of the local machine docker server. Whenever we run the eval $(minikube docker-env) , it actually exports couple of environment variable. We can see these environment variables by, minikube docker-env This should export the variables, that helps the docker client to determine which server it is going to connect. This command output also includes, # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) The second comment can guide us to configure the terminal to connect docker client with the virtual machine docker server.","title":"11 Multi Docker Access"},{"location":"Notes/02 Maintain Contianers/11 Multi Docker Access/#multi-docker-access","text":"We have two docker installed Inside local machine Inside the virtual machine, created by minikube By default our docker-client access the local docker-server . If we want to access the docker-server of the virtual machine , created by minikube , we can run, eval $(minikube docker-env) From this terminal, we can access the docker-server of the virtual machine. This is not a permanent re-configuration. This eval $(minikube docker-env) is a temporary connection of the virtual machine docker from the current terminal window. As soon as we go back to old terminal or open a new terminal, these terminal will make interaction of the local machine docker server. Whenever we run the eval $(minikube docker-env) , it actually exports couple of environment variable. We can see these environment variables by, minikube docker-env This should export the variables, that helps the docker client to determine which server it is going to connect. This command output also includes, # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) The second comment can guide us to configure the terminal to connect docker client with the virtual machine docker server.","title":"Multi Docker Access"},{"location":"Notes/02 Maintain Contianers/12 Requirement of Access Virtual Machine Docker Server/","text":"Requirement of Access Virtual Machine Docker Server There are couple of reasons, we might need to access the virtual machine docker server. Do debugging, check logs, executing arbitrary program inside the containers etc Testing k8s self healing by kill containers Removed the cached images from the node And many more...","title":"12 Requirement of Access Virtual Machine Docker Server"},{"location":"Notes/02 Maintain Contianers/12 Requirement of Access Virtual Machine Docker Server/#requirement-of-access-virtual-machine-docker-server","text":"There are couple of reasons, we might need to access the virtual machine docker server. Do debugging, check logs, executing arbitrary program inside the containers etc Testing k8s self healing by kill containers Removed the cached images from the node And many more...","title":"Requirement of Access Virtual Machine Docker Server"},{"location":"Notes/03 Multi Container App/- 01 Creating configurations/","text":"Creating Configurations To create a secret for the postgres password, kubectl create secret generic pgpassword --from-literal PGPASSWORD=1234asdf","title":"  01 Creating configurations"},{"location":"Notes/03 Multi Container App/- 01 Creating configurations/#creating-configurations","text":"To create a secret for the postgres password, kubectl create secret generic pgpassword --from-literal PGPASSWORD=1234asdf","title":"Creating Configurations"},{"location":"Notes/03 Multi Container App/- 02 Applying Configurations/","text":"Applying Configurations We are putting all the configurations in the k8s directory. With kubectl , instead of file, if we pass a directory as apply command, it will automatically look into the directory config files and apply them all on our behalf. Before applying, make sure minikube is running by minikube status . If not, start the minikube minikube start To apply all the config files inside k8s , we can run, kubectl apply -f k8s As output, we should see the list of created/configured/unchanged object. We can check the deployment objects, kubectl get deployments This should give us the output of deployment objects. We can check the pod objects, kubectl get pods This should give us the output of pod objects. We can take a look of the logs of container by, kubectl logs pod_name We can check the pod objects, kubectl get pods This should give us the output of pod objects.","title":"  02 Applying Configurations"},{"location":"Notes/03 Multi Container App/- 02 Applying Configurations/#applying-configurations","text":"We are putting all the configurations in the k8s directory. With kubectl , instead of file, if we pass a directory as apply command, it will automatically look into the directory config files and apply them all on our behalf. Before applying, make sure minikube is running by minikube status . If not, start the minikube minikube start To apply all the config files inside k8s , we can run, kubectl apply -f k8s As output, we should see the list of created/configured/unchanged object. We can check the deployment objects, kubectl get deployments This should give us the output of deployment objects. We can check the pod objects, kubectl get pods This should give us the output of pod objects. We can take a look of the logs of container by, kubectl logs pod_name We can check the pod objects, kubectl get pods This should give us the output of pod objects.","title":"Applying Configurations"},{"location":"Notes/03 Multi Container App/01 App Architecture/","text":"App Architecture","title":"01 App Architecture"},{"location":"Notes/03 Multi Container App/01 App Architecture/#app-architecture","text":"","title":"App Architecture"},{"location":"Notes/03 Multi Container App/02 Run Existing Application/","text":"Run Existing Application","title":"02 Run Existing Application"},{"location":"Notes/03 Multi Container App/02 Run Existing Application/#run-existing-application","text":"","title":"Run Existing Application"},{"location":"Notes/03 Multi Container App/03 Combining Config Files/","text":"Combining Config Files There's two ways we can organize the k8s config files in a directory. Put multiple config files in a single file : If we put multiple config files, we have to put a --- between two config files. With this architecture, we can put close objects configuration together. But the downside is, an engineer have to actively understand the tightly coupled services for any changes. Put each config file in individual file : In this case, each object config will have an individual file with meaningful name. So any time, if there is a requirement to change/update the config it is easy to find. The downside is, there could be a lot of files of configuration.","title":"03 Combining Config Files"},{"location":"Notes/03 Multi Container App/03 Combining Config Files/#combining-config-files","text":"There's two ways we can organize the k8s config files in a directory. Put multiple config files in a single file : If we put multiple config files, we have to put a --- between two config files. With this architecture, we can put close objects configuration together. But the downside is, an engineer have to actively understand the tightly coupled services for any changes. Put each config file in individual file : In this case, each object config will have an individual file with meaningful name. So any time, if there is a requirement to change/update the config it is easy to find. The downside is, there could be a lot of files of configuration.","title":"Combining Config Files"},{"location":"Notes/03 Multi Container App/04 Data Persistant/","text":"PVC When we crete a Deployment object with Postgres image, we create a Pod inside the virtual machine. The Postgres Container inside the Pod has a storage system, which is only accessible by the container itself. While the application running, Postgres used to store data in its container file system. Anytime the Pod crashed, the container along with its file system will be lost. So, after crashing a Pod with Postgres , if another Pod with Container get created, the new container can not access or represent the old crashed container data/files. PVC stands for persistent Volume Claims . This PVC thing is very much similar to the docker worlds volumes . In docker, we used to persist volume by Data Volume and Bind Mounting to make sure using/mapping the host machine storage from the container. We can come up architecture, where the Postgres will use the file system of the host machine. So in case the Pod crashed, and new Pod came up, the data in the host machine will not be lost. Also the new Pods Container can access these data. Volume In K8s : In docker world, we consider Volume as a file system inside the host machine, used/mapped by the container. But in k8s world, the Volume is an object, used to store data in the Pod level. In k8s when we create a Volume , we are creating a file system inside the Pod . The storage can be expressed as Belong To or Associated To the Pods . This Volume can be accessed by any container inside the Pod . Since Volume is tied with the Pods , if Pod dies, terminated or recreated, for any reason, the Volume itself will be gone as well. So, in k8s , the Volume is not appropriate for storing database. Persistent Volume : Persistent Volume is a long term durable storage, not tied to any Pod or Container . So, even a Pod or Container crashed, the data in the Persisted Volume survived. Persistent Volume Claim : Persistent Volume Claim is an advertizement of k8s for Statically provisioned persistent volume and Dynamically provisioned persistent volume . Statically provisioned persistent volume are created by k8s ahead of time but Dynamically provisioned persistent volume are created on demand on the fly. From Pods config file we can ask any of these and k8s will ensure these volumes on behalf of us. A Volume is tied to Pod and can not survived if the pod delete/recreated/crashed. On the other hand, the Persistent Volume survived even though the Pod can not survive. The Persistent Volume only lost, if the administrator removed it. Diagram Of Volume: single volume inside pod with multiple container pointing it Diagram of Persistent Volume: a volume outside of the Pod, pointing multiple container","title":"04 Data Persistant"},{"location":"Notes/03 Multi Container App/04 Data Persistant/#pvc","text":"When we crete a Deployment object with Postgres image, we create a Pod inside the virtual machine. The Postgres Container inside the Pod has a storage system, which is only accessible by the container itself. While the application running, Postgres used to store data in its container file system. Anytime the Pod crashed, the container along with its file system will be lost. So, after crashing a Pod with Postgres , if another Pod with Container get created, the new container can not access or represent the old crashed container data/files. PVC stands for persistent Volume Claims . This PVC thing is very much similar to the docker worlds volumes . In docker, we used to persist volume by Data Volume and Bind Mounting to make sure using/mapping the host machine storage from the container. We can come up architecture, where the Postgres will use the file system of the host machine. So in case the Pod crashed, and new Pod came up, the data in the host machine will not be lost. Also the new Pods Container can access these data. Volume In K8s : In docker world, we consider Volume as a file system inside the host machine, used/mapped by the container. But in k8s world, the Volume is an object, used to store data in the Pod level. In k8s when we create a Volume , we are creating a file system inside the Pod . The storage can be expressed as Belong To or Associated To the Pods . This Volume can be accessed by any container inside the Pod . Since Volume is tied with the Pods , if Pod dies, terminated or recreated, for any reason, the Volume itself will be gone as well. So, in k8s , the Volume is not appropriate for storing database. Persistent Volume : Persistent Volume is a long term durable storage, not tied to any Pod or Container . So, even a Pod or Container crashed, the data in the Persisted Volume survived. Persistent Volume Claim : Persistent Volume Claim is an advertizement of k8s for Statically provisioned persistent volume and Dynamically provisioned persistent volume . Statically provisioned persistent volume are created by k8s ahead of time but Dynamically provisioned persistent volume are created on demand on the fly. From Pods config file we can ask any of these and k8s will ensure these volumes on behalf of us. A Volume is tied to Pod and can not survived if the pod delete/recreated/crashed. On the other hand, the Persistent Volume survived even though the Pod can not survive. The Persistent Volume only lost, if the administrator removed it.","title":"PVC"},{"location":"Notes/03 Multi Container App/04 Data Persistant/#diagram-of-volume-single-volume-inside-pod-with-multiple-container-pointing-it","text":"","title":"Diagram Of Volume: single volume inside pod with multiple container pointing it"},{"location":"Notes/03 Multi Container App/04 Data Persistant/#diagram-of-persistent-volume-a-volume-outside-of-the-pod-pointing-multiple-container","text":"","title":"Diagram of Persistent Volume: a volume outside of the Pod, pointing multiple container"},{"location":"Notes/k8s Deployments/01 Deployment Process/","text":"","title":"01 Deployment Process"},{"location":"Notes/k8s Deployments/02 GCP vs AWS for K8s/","text":"","title":"02 GCP vs AWS for K8s"},{"location":"Notes/resource/blogs/Objects in K8s World/content/","text":"Kubernetes is out there to orchestrate our containers to a desired state. We notify the kubernetes engine our desired application state with a configuration files. We feed these configuration files to kubectl and kubectl pass them to the master or the control plane . Here theses config files becomes object/controller and helps the master to keep the application always in the desired state. According to the Kubernetes Doc , Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. In k8s there are couple types of objects, for example, StatefulSet ReplicaController Pod Service etc. These objects have different purposes, like, Running a container Monitoring Setting up networking etc. In this article we will discuss very commonly used object types, Pod , Service and Secret . Pod Object Types Whenever we start the minikube by minikube start , we started a virtual machine in the local machine. We call this virtual machine Node in terms of k8s . Pod is one of the most basic objects. Pod essentially is a grouping of containers of very similar purpose. Containers in a single Pod must be very tightly coupled and must be executed with each other. For example, if we have an application with API container and frontend container, we might not want to put them in a single Pod . But if there is a container for DB and another for Logging the DB , in this case both container should go inside a same Pod . In k8s we can not run a bare bone container without any overhead associate. A Pod is the minimum overhead to deploy a container in the k8s cluster. Inside Pod we have to run at least one or more container in it. When we push the Pod config file using kubectl to the k8s engine, it eventually create a Pod inside the Node aka Virtual Machine . We define which container will run inside the Pod , in the config file under spec -> containers . Service Object Types Service is all about setting up networking in the k8s cluster . Service types have 4 sub-types, NodePort ClusterIP LoadBalancer Ingress We define the sub-types under the spec property. NodePort NodePort is supposed to expose the container to the outside world. In most cases, we use NodePort inside the dev stage. There's a selector under the spec also. In the Service config file there is no reference of the Pod config file. In the Pod config file, we have a name and labels under the metadata . To send traffic to the Pod from the Service , we do not see any reference of the Pod . Instead in k8s , we use the label selector mechanism. In the Service file we have a selector under component: web , which is same as the Pod file labels, component: web . This is how the Service knows the Pod to send traffic. After selecting the Pod from the selector , comes the ports, under the spec -> NodePort -> ports we got 3 types of ports, ports : Internally other Pod / object connect through this port. targetPort : This should be same as the containerPort in the Pod config file. This is the open port of the container inside the Pod . nodePort : This is the port our application is exposed to the outside world. This port ranges 30000 to 32767. If we do not specify, it will generate a random port within this range. ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects ClusterIP It is a restrictive form of networking. It allows any objects inside the cluster to access the object it is pointed to. But outside ot the cluster, like from browser, it does not allow to access that object. Practically, ClusterIP allows anyone to access the object. Without this service, the object can not be accessed and inf we use NodePort service instead, it will expose the object to the outside world of the k8s cluster. It has 2 types of port, Port : In the k8s cluster , other objects can connect to the clusterIp service container using this port. targetPort : The ClientIP Service is pointing to the container with this targetPort . ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects Load Balancer This is an older way to handle traffic from outside to k8s cluster. With Load Balance Service , we replace the ClusterIp Service with it. And the k8s reach to the cloud provider to get there built in load balancer (For example, in AWS, it could be Classic Load Balancer or Application Load Balancer). When we use Load Balancer Service , it only allows the k8s cluster to expose only a set of specific Pods. The limitations are, it can not expose multiple set of pods. Ingress Service This Ingress service allows to come traffic inside the k8s cluster . When the traffic is inside the k8s cluster , effectively these traffic can also access the objects through ClusterIP` service. Ingress Service When it comes to ingress service in k8s, there are two options we have to consider, kubernetes-ingress : Maintained by the Nginx company itself. ingress-nginx : A community driven project. Comparatively, ingress nginx can be a better choice , we will use and discuss about the ingress-nginx here. Difference between them are discussed here . When it comes to set up ingress-nginx , it varies for different cloud provider. Controller : In k8s a controller is a kind of object, that constantly works to make the current state to the desired state. For example, when we create a Deployment Object using the config file, the Deployment Object observe the k8s cluster current state and ensure it to match the desired state. In a ingress service , we create a config file with some routing rules for the incoming traffic and send it off to the appropriate service inside the cluster. Now the kubectl create the ingress controller . Internally this ingress service is using Nginx . So this ingress controller will create a Pod with Nginx Controller in it. With this Nginx container the routes being handled and passed to different set of Pods . Ingress Nginx In Google Cloud As always, we have to create the config file of the ingress service . This config will be passed to a Deployment where the ingress controller and the Nginx Pod both will run. This Nginx Pod will take the incoming traffic and pass them to the appropriate set of Pods . In Google Cloud , when we create a ingress service , a load balancer Google Cloud Load Balancer will also be created for us. This Google Cloud Load Balancer is a cloud native service. When the Google Cloud Load Balancer is created outside of the cluster, a Load Balancer Service will also be created inside the k8s cluster . We know the Load Balancer Service is only capable of distribute traffic only a set of Pods . So here the Load Balancer Service will pass all these traffic to Nginx Pod and Nginx Pod will take care of the rest of traffic distribution between Pods . In dev, there is an additional deployment object, default-backend-pod , to ensure the health check of the other Pods . In production this will be replaced by the original application. Why ingress-nginx? We can simply make use of a Google Cloud Load Balancer , Load Balancer Service and Plain Nginx Pod . But we make use of the ingress-nginx because it is optimized and built for run in such k8s cluster environment. For example, when we are using Nginx Pod , it can bypass the Cluster Ip Service and pass traffic directly to the Pod . This feature is required for sticky session . Sticky Session enables the user to a client always communicate with the same server. Enabling Ingress To enable the ingress service in minikube , minikube addons enable ingress In output, we should see The 'ingress' addon is enabled . To verify, if the ingress service is enabled or not, kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch To detect, which version is being installed, nginx-ingress-controller --version Secret Object Types Since We should not write the secret in file/configuration, When we pass secret to the k8s it will be a imperative deployment. Secret itself is a object type. Instead of write a config file, we will use kubectl to create this secret object. 3 Types Of Secret generic : Regular application secrets, like DB_PASSWORD, JWT_SECRET, API_SECRET etc. docker-registry : Create secrets to use for the Docker Registry tls : Used to store certificates and associated keys When we pass --from-literal , it means we are passing the secret key and value in inline format. Another option can be load the secrets from a file, which is not ideal to load secrets. When we are using this Imperative Deployment to create secret, we have to execute the command for both in local environment and production cloud environment. To create a Secret object, we use, kubectl create secret <secret_type> <secret_name> --from-literal <key=value> This should give us output, secret/<secret_name> created . We can get the list of secrets, kubectl get secrets","title":"Content"},{"location":"Notes/resource/blogs/Objects in K8s World/content/#pod-object-types","text":"Whenever we start the minikube by minikube start , we started a virtual machine in the local machine. We call this virtual machine Node in terms of k8s . Pod is one of the most basic objects. Pod essentially is a grouping of containers of very similar purpose. Containers in a single Pod must be very tightly coupled and must be executed with each other. For example, if we have an application with API container and frontend container, we might not want to put them in a single Pod . But if there is a container for DB and another for Logging the DB , in this case both container should go inside a same Pod . In k8s we can not run a bare bone container without any overhead associate. A Pod is the minimum overhead to deploy a container in the k8s cluster. Inside Pod we have to run at least one or more container in it. When we push the Pod config file using kubectl to the k8s engine, it eventually create a Pod inside the Node aka Virtual Machine . We define which container will run inside the Pod , in the config file under spec -> containers .","title":"Pod Object Types"},{"location":"Notes/resource/blogs/Objects in K8s World/content/#service-object-types","text":"Service is all about setting up networking in the k8s cluster . Service types have 4 sub-types, NodePort ClusterIP LoadBalancer Ingress We define the sub-types under the spec property. NodePort NodePort is supposed to expose the container to the outside world. In most cases, we use NodePort inside the dev stage. There's a selector under the spec also. In the Service config file there is no reference of the Pod config file. In the Pod config file, we have a name and labels under the metadata . To send traffic to the Pod from the Service , we do not see any reference of the Pod . Instead in k8s , we use the label selector mechanism. In the Service file we have a selector under component: web , which is same as the Pod file labels, component: web . This is how the Service knows the Pod to send traffic. After selecting the Pod from the selector , comes the ports, under the spec -> NodePort -> ports we got 3 types of ports, ports : Internally other Pod / object connect through this port. targetPort : This should be same as the containerPort in the Pod config file. This is the open port of the container inside the Pod . nodePort : This is the port our application is exposed to the outside world. This port ranges 30000 to 32767. If we do not specify, it will generate a random port within this range. ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects ClusterIP It is a restrictive form of networking. It allows any objects inside the cluster to access the object it is pointed to. But outside ot the cluster, like from browser, it does not allow to access that object. Practically, ClusterIP allows anyone to access the object. Without this service, the object can not be accessed and inf we use NodePort service instead, it will expose the object to the outside world of the k8s cluster. It has 2 types of port, Port : In the k8s cluster , other objects can connect to the clusterIp service container using this port. targetPort : The ClientIP Service is pointing to the container with this targetPort . ** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects Load Balancer This is an older way to handle traffic from outside to k8s cluster. With Load Balance Service , we replace the ClusterIp Service with it. And the k8s reach to the cloud provider to get there built in load balancer (For example, in AWS, it could be Classic Load Balancer or Application Load Balancer). When we use Load Balancer Service , it only allows the k8s cluster to expose only a set of specific Pods. The limitations are, it can not expose multiple set of pods. Ingress Service This Ingress service allows to come traffic inside the k8s cluster . When the traffic is inside the k8s cluster , effectively these traffic can also access the objects through ClusterIP` service.","title":"Service Object Types"},{"location":"Notes/resource/blogs/Objects in K8s World/content/#ingress-service","text":"When it comes to ingress service in k8s, there are two options we have to consider, kubernetes-ingress : Maintained by the Nginx company itself. ingress-nginx : A community driven project. Comparatively, ingress nginx can be a better choice , we will use and discuss about the ingress-nginx here. Difference between them are discussed here . When it comes to set up ingress-nginx , it varies for different cloud provider. Controller : In k8s a controller is a kind of object, that constantly works to make the current state to the desired state. For example, when we create a Deployment Object using the config file, the Deployment Object observe the k8s cluster current state and ensure it to match the desired state. In a ingress service , we create a config file with some routing rules for the incoming traffic and send it off to the appropriate service inside the cluster. Now the kubectl create the ingress controller . Internally this ingress service is using Nginx . So this ingress controller will create a Pod with Nginx Controller in it. With this Nginx container the routes being handled and passed to different set of Pods . Ingress Nginx In Google Cloud As always, we have to create the config file of the ingress service . This config will be passed to a Deployment where the ingress controller and the Nginx Pod both will run. This Nginx Pod will take the incoming traffic and pass them to the appropriate set of Pods . In Google Cloud , when we create a ingress service , a load balancer Google Cloud Load Balancer will also be created for us. This Google Cloud Load Balancer is a cloud native service. When the Google Cloud Load Balancer is created outside of the cluster, a Load Balancer Service will also be created inside the k8s cluster . We know the Load Balancer Service is only capable of distribute traffic only a set of Pods . So here the Load Balancer Service will pass all these traffic to Nginx Pod and Nginx Pod will take care of the rest of traffic distribution between Pods . In dev, there is an additional deployment object, default-backend-pod , to ensure the health check of the other Pods . In production this will be replaced by the original application. Why ingress-nginx? We can simply make use of a Google Cloud Load Balancer , Load Balancer Service and Plain Nginx Pod . But we make use of the ingress-nginx because it is optimized and built for run in such k8s cluster environment. For example, when we are using Nginx Pod , it can bypass the Cluster Ip Service and pass traffic directly to the Pod . This feature is required for sticky session . Sticky Session enables the user to a client always communicate with the same server. Enabling Ingress To enable the ingress service in minikube , minikube addons enable ingress In output, we should see The 'ingress' addon is enabled . To verify, if the ingress service is enabled or not, kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch To detect, which version is being installed, nginx-ingress-controller --version","title":"Ingress Service"},{"location":"Notes/resource/blogs/Objects in K8s World/content/#secret-object-types","text":"Since We should not write the secret in file/configuration, When we pass secret to the k8s it will be a imperative deployment. Secret itself is a object type. Instead of write a config file, we will use kubectl to create this secret object. 3 Types Of Secret generic : Regular application secrets, like DB_PASSWORD, JWT_SECRET, API_SECRET etc. docker-registry : Create secrets to use for the Docker Registry tls : Used to store certificates and associated keys When we pass --from-literal , it means we are passing the secret key and value in inline format. Another option can be load the secrets from a file, which is not ideal to load secrets. When we are using this Imperative Deployment to create secret, we have to execute the command for both in local environment and production cloud environment. To create a Secret object, we use, kubectl create secret <secret_type> <secret_name> --from-literal <key=value> This should give us output, secret/<secret_name> created . We can get the list of secrets, kubectl get secrets","title":"Secret Object Types"},{"location":"Notes/resource/blogs/Towards K8s/content/","text":"Before we go further, we will try to find out the answers two most important question, What is Kubernetes? Why we use Kubernetes? In short form, we can refer Kubernetes as k8s . We can spell k-eight-s . Eight is here to refer eight letter between first and last letter of kubernetes . Let's assume we a running multiple containers in the Elastic Beanstalk . If we started to get a lot of users to our application, we might need to scale the application. When we do the scaling, it is important to keep in mind, we only scale the container who took the loads of the traffic, not all the containers. But with Elastic Beanstalk this type of scaling will be real challenging. Let's find out how k8s can solve this entire scaling issue. In k8s, there is k8s cluster that is made up with master + nodes . Here, nodes can be virtual machine or physical computer contains some number of different containers. And, master controls each of the nodes. So with a k8s cluster , we can have a node that will contain some containers these do not have to scale. And a bunch of other nodes contains the worker process container, need to scale with traffics, they will be scaled according to the traffic. The master is running couple of programs to determine how the nodes will run all the containers on our behalf. As developer, we interact the k8s cluster only by the master . We send the instructions to the master and the master will pass all these instructions to the respective nodes . So the bottom line is, we can summarize the answer What is k8s? A container orchestrator make many servers act like one. Kubernetes is one of the most popular container orchestrator. Why we use k8s? Deploy the containerized apps. k8s Tools kubernetes : K8s is the short form of kubernetes . It's the whole orchestration system. kubectl : kubectl is an abbreviation of kube control . It's the CLI for k8s and used to configure and manage the apps. Nodes : Single server/virtual machine in the k8s cluster. kubelet : An small agent, run in the Node to communicate between Node and the master . Control Plane : Also known as master . Control Panel or master is in the charge of whole cluster. It make sure, the current state of the cluster is matched with the desired state. Local Environment for k8s (Ubuntu) To run k8s in the local environment, we will need two programs installed in the machine, Minikube kubectl Install Minikube Download the program, curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 To install, sudo install minikube-linux-amd64 /usr/local/bin/minikube This is optional. If the user is not added to the docker group , sudo usermod -aG docker $USER && newgrp docker Start Minikube by minikube start To check if minikube is running, check the status, minikube status Install kubectl Download the program, curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" To install sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl To test if the kubectl is installed, check the version, kubectl version Verify Local Machine Setup We will run an docker image in the k8s cluster. Check the minikube status, minikube status We should see the status as running and configured. To test the cluster, kubectl cluster-info We should see the cluster is running. For any error/warning please check the installation section. k8s Development Flow In the k8s world, we pass the config files like Pod or Service to the master using the kubectl . The master has a responsibility to determine how many container is being running inside the pods in various nodes. We can define which nodes will run which container and how many. The master monitor all these containers inside the Pod of the Node . If some nodes fails to run or crash on runtime, the master will run another to keep the expectation. Imperative vs Declarative Deployments Whenever we need to update/interact with a k8s cluster, we update/change the desired state in a config file and pass it to master. We should never directly go to the nodes directly and and update the nodes/pods/container. The master is constantly working to meet the desired state. With Imperative Deployments , we have to provide specific instructions step by step to reach a objective. But when we go through the Declarative Deployments , we only define the final objective to the master and master will make it happen. Whenever it comes to update status it's always good practice to update the config file and feed it to the master and follow Declarative Deployments . k8s in Production When we choose to use k8s as orchestrator, then we need to choose the vendors for it. We can go either Cloud Managed Solution like GCP , AWS , Azure , Digital Ocean etc. We can also choose self managed distribution like, Docker Enterprise , Rancher , OpenShift , Canonical , VMWARE PKS etc.","title":"Content"},{"location":"Notes/resource/blogs/Towards K8s/content/#k8s-tools","text":"kubernetes : K8s is the short form of kubernetes . It's the whole orchestration system. kubectl : kubectl is an abbreviation of kube control . It's the CLI for k8s and used to configure and manage the apps. Nodes : Single server/virtual machine in the k8s cluster. kubelet : An small agent, run in the Node to communicate between Node and the master . Control Plane : Also known as master . Control Panel or master is in the charge of whole cluster. It make sure, the current state of the cluster is matched with the desired state.","title":"k8s Tools"},{"location":"Notes/resource/blogs/Towards K8s/content/#local-environment-for-k8s-ubuntu","text":"To run k8s in the local environment, we will need two programs installed in the machine, Minikube kubectl Install Minikube Download the program, curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 To install, sudo install minikube-linux-amd64 /usr/local/bin/minikube This is optional. If the user is not added to the docker group , sudo usermod -aG docker $USER && newgrp docker Start Minikube by minikube start To check if minikube is running, check the status, minikube status Install kubectl Download the program, curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" To install sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl To test if the kubectl is installed, check the version, kubectl version","title":"Local Environment for k8s (Ubuntu)"},{"location":"Notes/resource/blogs/Towards K8s/content/#verify-local-machine-setup","text":"We will run an docker image in the k8s cluster. Check the minikube status, minikube status We should see the status as running and configured. To test the cluster, kubectl cluster-info We should see the cluster is running. For any error/warning please check the installation section.","title":"Verify Local Machine Setup"},{"location":"Notes/resource/blogs/Towards K8s/content/#k8s-development-flow","text":"In the k8s world, we pass the config files like Pod or Service to the master using the kubectl . The master has a responsibility to determine how many container is being running inside the pods in various nodes. We can define which nodes will run which container and how many. The master monitor all these containers inside the Pod of the Node . If some nodes fails to run or crash on runtime, the master will run another to keep the expectation.","title":"k8s Development Flow"},{"location":"Notes/resource/blogs/Towards K8s/content/#imperative-vs-declarative-deployments","text":"Whenever we need to update/interact with a k8s cluster, we update/change the desired state in a config file and pass it to master. We should never directly go to the nodes directly and and update the nodes/pods/container. The master is constantly working to meet the desired state. With Imperative Deployments , we have to provide specific instructions step by step to reach a objective. But when we go through the Declarative Deployments , we only define the final objective to the master and master will make it happen. Whenever it comes to update status it's always good practice to update the config file and feed it to the master and follow Declarative Deployments .","title":"Imperative vs Declarative Deployments"},{"location":"Notes/resource/blogs/Towards K8s/content/#k8s-in-production","text":"When we choose to use k8s as orchestrator, then we need to choose the vendors for it. We can go either Cloud Managed Solution like GCP , AWS , Azure , Digital Ocean etc. We can also choose self managed distribution like, Docker Enterprise , Rancher , OpenShift , Canonical , VMWARE PKS etc.","title":"k8s in Production"},{"location":"Notes/resource/completed-code/client/","text":"This project was bootstrapped with Create React App . Available Scripts In the project directory, you can run: npm start Runs the app in the development mode. Open http://localhost:3000 to view it in the browser. The page will reload if you make edits. You will also see any lint errors in the console. npm test Launches the test runner in the interactive watch mode. See the section about running tests for more information. npm run build Builds the app for production to the build folder. It correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed! See the section about deployment for more information. npm run eject Note: this is a one-way operation. Once you eject , you can\u2019t go back! If you aren\u2019t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own. You don\u2019t have to ever use eject . The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it. Learn More You can learn more in the Create React App documentation . To learn React, check out the React documentation . Code Splitting This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting Analyzing the Bundle Size This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size Making a Progressive Web App This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app Advanced Configuration This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration Deployment This section has moved here: https://facebook.github.io/create-react-app/docs/deployment npm run build fails to minify This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"Index"},{"location":"Notes/resource/completed-code/client/#available-scripts","text":"In the project directory, you can run:","title":"Available Scripts"},{"location":"Notes/resource/completed-code/client/#npm-start","text":"Runs the app in the development mode. Open http://localhost:3000 to view it in the browser. The page will reload if you make edits. You will also see any lint errors in the console.","title":"npm start"},{"location":"Notes/resource/completed-code/client/#npm-test","text":"Launches the test runner in the interactive watch mode. See the section about running tests for more information.","title":"npm test"},{"location":"Notes/resource/completed-code/client/#npm-run-build","text":"Builds the app for production to the build folder. It correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed! See the section about deployment for more information.","title":"npm run build"},{"location":"Notes/resource/completed-code/client/#npm-run-eject","text":"Note: this is a one-way operation. Once you eject , you can\u2019t go back! If you aren\u2019t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own. You don\u2019t have to ever use eject . The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it.","title":"npm run eject"},{"location":"Notes/resource/completed-code/client/#learn-more","text":"You can learn more in the Create React App documentation . To learn React, check out the React documentation .","title":"Learn More"},{"location":"Notes/resource/completed-code/client/#code-splitting","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting","title":"Code Splitting"},{"location":"Notes/resource/completed-code/client/#analyzing-the-bundle-size","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size","title":"Analyzing the Bundle Size"},{"location":"Notes/resource/completed-code/client/#making-a-progressive-web-app","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app","title":"Making a Progressive Web App"},{"location":"Notes/resource/completed-code/client/#advanced-configuration","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration","title":"Advanced Configuration"},{"location":"Notes/resource/completed-code/client/#deployment","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/deployment","title":"Deployment"},{"location":"Notes/resource/completed-code/client/#npm-run-build-fails-to-minify","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"npm run build fails to minify"}]}