[{"type":"folder","name":"01 Onwards to k8s","children":[{"type":"file","name":"01 Why k8s","children":null,"path":"../..//01 Onwards to k8s/01 Why k8s.md","content":"### Why k8s\n\n---\n\nBefore we go further, we will try to find out the answers two most important question,\n\n- What is k8s?\n- Why we use k8s?\n\nLet's assume we a running multiple containers in the `Elastic Beanstalk`. If we started to get a lot of users to our application, we might need to scale the application. When we do the scaling, it is important to keep in mind, we only scale the container of the worker process, not all the container. But with `Elastic Beanstalk` this type of scaling will be real challenging.\n\nLet's find out how k8s can solve this entire scaling issue.\n\nIn k8s there is k8s cluster that is made up with `master + nodes`.\n\nHere, **nodes** can be virtual machine or physical computer contains some number of different containers. And, **master** controls each of the nodes.\n\nSo with a `k8s cluster`, we can have a `node` that will contain some containers these do not have to scale. And a bunch of other `nodes` contains the worker process container, they will be scaled according to the traffic.\n\nThe `master` is running couple of programs to determine how the nodes will run all the containers on our behalf. As developer, we interact the `k8s cluster` only by the `master`. We send the instructions to the `master` and the `master` will pass all these instructions to the respective `nodes`.\n\nThere will be a `Load Balance` in front of the `k8s cluster`.\n\nSo the bottom line is, we can summarize the answer\n\n- What is k8s?\n\n> A system to run multiple containers in multiple machines (nodes).\n\n- Why we use k8s?\n\n> Deploy the containerized apps.\n"},{"type":"file","name":"02 Local Environment for k8s","children":null,"path":"../..//01 Onwards to k8s/02 Local Environment for k8s.md","content":"### Local Environment for k8s\n\n---\n\nTo run k8s in the local environment, we will need two programs installed in the machine,\n\n- Minikube\n- kubectl\n\n**Install Minikube**\n\nDownload the program,\n\n```bash\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n```\n\nTo install,\n\n```bash\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n```\n\nThis is optional. If the user is not added to the `docker group`,\n\n```bash\nsudo usermod -aG docker $USER && newgrp docker\n```\n\nStart Minikube by\n\n```bash\nminikube start\n```\n\nTo check if minikube is running, check the status,\n\n```bash\nminikube status\n```\n\n**Install kubectl**\n\nDownload the program,\n\n```bash\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n```\n\nTo install\n\n```bash\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n```\n\nTo test if the `kubectl` is installed, check the version,\n\n```bash\nkubectl version\n```\n"},{"type":"file","name":"03 Verify Local Machine Setup","children":null,"path":"../..//01 Onwards to k8s/03 Verify Local Machine Setup.md","content":"### Verify Local Machine Setup\n\n---\n\nWe will run an docker image in the `k8s` cluster.\n\nCheck the `minikube` status,\n\n```bash\nminikube status\n```\n\nWe should see the status as running and configured.\n\nTo test the cluster,\n\n```bash\nkubectl cluster-info\n```\n\nWe should see the cluster is running.\n\nFor any error/warning please check the installation section.\n"},{"type":"file","name":"04 Object Types And API Versions","children":null,"path":"../..//01 Onwards to k8s/04 Object Types And API Versions.md","content":"### Object Types And API Versions\n\n---\n\nIn `k8s` world, apart from `docker-compose`, we need to consider,\n\n- `k8s` expect all the containers image already be built\n- In `k8s cluster` we have to manually do the networking\n\nOur `client-pod.yml` should be like the following,\n\n```yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: proxy-pod\n  labels:\n    component: web\nspec:\n  containers:\n    - name: nginx-proxy\n      image: nginx\n      ports:\n        - containerPort: 80\n```\n\nAnd our `client-node-port.yml` should be like the following,\n\n```yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: proxy-node-port\nspec:\n  type: NodePort\n  ports:\n    - port: 3050\n      targetPort: 80\n      nodePort: 31516\n  selector:\n    component: web\n```\n\nIn `k8s` world, when we are making a config file, we are not essentially making a `container`, instead we are making a object. These object/thing is being created inside the `k8s` engine to make the\n\n**kind**\n\nWe made two configuration file, `client-pod.yml` and `client-node-port.yml`. We will feed these two configuration file to the `kubectl`. `kubectl` will take these two config file and create two objects out of them. In `k8s` there are couple types of objects, for example,\n\n- StatefulSet\n- ReplicaController\n- Pod\n- Service etc.\n\nThese objects have different purposes, like,\n\n- Running a container\n- Monitoring\n- Setting up networking etc.\n\nIn our config file, we define the types of the object by the `kind` property. For example, in the `client-pod.yml` file, we set the object type `Pod` in the `yml` file by `kind: Pod`. similarly, in the `client-node-port.yml` file, we define the object type to `Service` by `kind: Service`.\n\n**apiVersion**\n\nEach `apiVersion` has a set of object types. For example, when the `apiVersion` is `v1`, we can specify the `objectTypes` or `kind` as,\n\n- componentStatus\n- configMap\n- Endpoints\n- Event\n- Namespace\n- Pod\n- PersistentVolumeClaim\n- Secrets\n\nInstead, if the `apiVersion` is `apps/v1`, we can use `objectTypes` or `kind` as,\n\n- ControllerRevision\n- StatefulSet\n\n> Each API version has different set of objects type. So before we using any object types, we have to check the api Version and specify on the top of the `yml` file.\n\n### Persistent Volume Access Modes\n\n---\n\nIn k8s there are 3 types of access modes for the persistent volumes,\n\n1. **ReadWriteOnce**: Only one node is allowed to do read/write operation in the volume.\n2. **ReadOnlyMany**: Multiple nodes can read from the volume at the same time.\n3. **ReadWriteMany**: Multiple nodes can perform both operation, read and write to the volume at the same time.\n\n### Allocating Persistent Volume\n\n---\n\nWhen we ask k8s for persistent volume, it reaches for some storage class. In local machine, the default storage class is a slice of the hard disk. This is a reason, we do not need to specify the storage class in local machine. We can see the list of storage class,\n\n```bash\nkubectl get storageclass\n```\n\nWe can get details of the storage class by,\n\n```bash\nkubectl describe storageclass\n```\n\nWhen we run the k8s in cloud, then we have tons of options for storage class. For example, we can use `AWS EBS`, `Google Cloud Persistence Disk`, `Azure File`, `Azure Disk` etc. For each cloud provider, a default storage class is automatically configured.\n\nWe can see the list of persistence volumes,\n\n```bash\nkubectl get pv\n```\n\nTo see the persistent volume claims,\n\n```bash\nkubectl get pvc\n```\n\nThis `PVC` list is showing, we can use these persistence volume. And the `PV` list is the actual use cases of these volumes.\n"},{"type":"file","name":"05 Pod Object Types","children":null,"path":"../..//01 Onwards to k8s/05 Pod Object Types.md","content":"### Pod Object Types\n\n---\n\nWhenever we start the `minikube` by `minikube start`, we started a virtual machine in the local machine. We call this virtual machine `Node` in terms of `k8s`.\n\n`Pod` is one of the most basic objects. `Pod` essentially is a grouping of containers of very similar purpose. Containers in a single `Pod` must be very tightly coupled and must be executed with each other. For example, if we have an application with `API` container and `frontend` container, we might not want to put them in a single `Pod`. But if there is a container for `DB` and another for `Logging the DB`, in this case both container should go inside a same `Pod`.\n\nIn `k8s` we can not run a bare bone container without any overhead associate. A `Pod` is the minimum overhead to deploy a container in the `k8s` cluster. Inside `Pod` we have to run at least one or more container in it.\n\nWhen we push the `Pod` config file using `kubectl` to the `k8s` engine, it eventually create a `Pod` inside the `Node` aka `Virtual Machine`.\n\nWe define which container will run inside the `Pod`, in the config file under `spec -> containers`.\n"},{"type":"file","name":"06 Service Object Types","children":null,"path":"../..//01 Onwards to k8s/06 Service Object Types.md","content":"### Service Object Types\n\n---\n\n`Service` is all about setting up networking in the `k8s cluster`. `Service` types have 4 sub-types,\n\n- NodePort\n- ClusterIP\n- LoadBalancer\n- Ingress\n\nWe define the sub-types under the `spec` property.\n\n<u>**NodePort**</u>\n\n`NodePort` is supposed to expose the container to the outside world. In most cases, we use `NodePort` inside the dev stage.\n\nThere's a `selector` under the `spec` also. In the `Service` config file there is no reference of the `Pod` config file. In the `Pod` config file, we have a `name` and `labels` under the `metadata`. To send traffic to the `Pod` from the `Service`, we do not see any reference of the `Pod`. Instead in `k8s`, we use the `label selector` mechanism. In the `Service` file we have a selector under `component: web`, which is same as the `Pod` file labels, `component: web`. This is how the `Service` knows the `Pod` to send traffic.\n\nAfter selecting the `Pod` from the `selector`, comes the ports, under the `spec -> NodePort -> ports` we got 3 types of ports,\n\n1. **ports**: Internally other `Pod`/`object` connect through this port.\n2. **targetPort**: This should be same as the `containerPort` in the `Pod` config file. This is the open port of the container inside the `Pod`.\n3. **nodePort**: This is the port our application is exposed to the outside world. This port ranges 30000 to 32767. If we do not specify, it will generate a random port within this range.\n\n**\\*\\*** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects\n\n<u>**ClusterIP**</u>\n\nIt is a restrictive form of networking. It allows any objects inside the cluster to access the object it is pointed to. But outside ot the cluster, like from browser, it does not allow to access that object.\n\nPractically, `ClusterIP` allows anyone to access the object. Without this service, the object can not be accessed and inf we use `NodePort` service instead, it will expose the object to the outside world of the k8s cluster.\n\nIt has 2 types of port,\n\n1. **Port**: In the `k8s cluster`, other objects can connect to the clusterIp service container using this port.\n2. **targetPort**: The `ClientIP Service` is pointing to the container with this `targetPort`.\n\n**\\*\\*** Diagram of port mapping of theses 3 ports by local machine, cluster, container, other objects\n\n<u>**Load Balancer**</u>\n\nThis is an older way to handle traffic from outside to k8s cluster. With `Load Balance Service`, we replace the `ClusterIp Service` with it. And the k8s reach to the cloud provider to get there built in load balancer (For example, in AWS, it could be Classic Load Balancer or Application Load Balancer). When we use `Load Balancer Service`, it only allows the k8s cluster to expose only a set of specific Pods. The limitations are, it can not expose multiple set of pods.\n\n<u>**Ingress Service**</u>\n\nThis Ingress service allows to come traffic inside the `k8s cluster`. When the traffic is inside the `k8s cluster`, effectively these traffic can also access the objects through ClusterIP` service.\n"},{"type":"file","name":"07 Ingress Service","children":null,"path":"../..//01 Onwards to k8s/07 Ingress Service.md","content":"### Ingress Service\n\n---\n\nWhen it comes to ingress service in k8s, there are two options we have to consider,\n\n1. [kubernetes-ingress](https://github.com/nginxinc/kubernetes-ingress): Maintained by the `Nginx` company itself.\n2. [ingress-nginx](https://github.com/kubernetes/ingress-nginx): A community driven project.\n\nComparatively, ingress nginx can be a [better choice](https://www.joyfulbikeshedding.com/blog/2018-03-26-studying-the-kubernetes-ingress-system.html), we will use and discuss about the `ingress-nginx` here.\n\nDifference between them are discussed [here](https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md).\n\nWhen it comes to set up `ingress-nginx`, it varies for different cloud provider.\n\n**Controller**: In k8s a controller is a kind of object, that constantly works to make the current state to the desired state. For example, when we create a `Deployment Object` using the config file, the `Deployment Object` observe the k8s cluster current state and ensure it to match the desired state.\n\nIn a `ingress service`, we create a config file with some routing rules for the incoming traffic and send it off to the appropriate service inside the cluster. Now the `kubectl` create the `ingress controller`. Internally this `ingress service` is using `Nginx`. So this `ingress controller` will create a `Pod` with `Nginx Controller` in it. With this `Nginx` container the routes being handled and passed to different set of `Pods`.\n\n**Ingress Nginx In Google Cloud**\n\nAs always, we have to create the config file of the `ingress service`. This config will be passed to a `Deployment` where the `ingress controller` and the `Nginx Pod` both will run. This `Nginx Pod` will take the incoming traffic and pass them to the appropriate set of `Pods`. In `Google Cloud`, when we create a `ingress service`, a load balancer `Google Cloud Load Balancer` will also be created for us. This `Google Cloud Load Balancer` is a cloud native service. When the `Google Cloud Load Balancer` is created outside of the cluster, a `Load Balancer Service` will also be created inside the `k8s cluster`.\n\nWe know the `Load Balancer Service` is only capable of distribute traffic only a set of `Pods`. So here the `Load Balancer Service` will pass all these traffic to `Nginx Pod` and `Nginx Pod` will take care of the rest of traffic distribution between `Pods`.\n\nIn dev, there is an additional deployment object, `default-backend-pod`, to ensure the health check of the other `Pods`. In production this will be replaced by the original application.\n\n**Why ingress-nginx?**\n\nWe can simply make use of a `Google Cloud Load Balancer`, `Load Balancer Service` and `Plain Nginx Pod`. But we make use of the `ingress-nginx` because it is optimized and built for run in such `k8s cluster` environment. For example, when we are using `Nginx Pod`, it can bypass the `Cluster Ip Service` and pass traffic directly to the `Pod`. This feature is required for `sticky session`.\n\n> `Sticky Session` enables the user to a client always communicate with the same server.\n\n**[Enabling Ingress](https://kubernetes.github.io/ingress-nginx/deploy/#verify-installation)**\n\nTo enable the `ingress` service in `minikube`,\n\n```bash\nminikube addons enable ingress\n```\n\nIn output, we should see `The 'ingress' addon is enabled`.\n\nTo verify, if the `ingress` service is enabled or not,\n\n```bash\nkubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch\n```\n\nTo detect, which version is being installed,\n\n```bash\nnginx-ingress-controller --version\n```\n"},{"type":"file","name":"08 Secret Object Types","children":null,"path":"../..//01 Onwards to k8s/08 Secret Object Types.md","content":"### Secret Object Types\n\n---\n\nSince We should not write the secret in file/configuration, When we pass secret to the k8s it will be a imperative deployment.\n\n`Secret` itself is a object type. Instead of write a config file, we will use `kubectl` to create this secret object.\n\n**3 Types Of Secret**\n\n1. **generic**:\n2. **docker-registry**:\n3. **tls**:\n\nWhen we pass `--from-literal`, it means we are passing the secret key and value in inline format. Another option can be load the secrets from a file, which is not ideal to load secrets.\n\nWhen we are using this `Imperative Deployment` to create secret, we have to execute the command for both in local environment and production cloud environment.\n\nTo create a `Secret` object, we use,\n\n```bash\nkubectl create secret <secret_type> <secret_name> --from-literal <key=value>\n```\n\nThis should give us output, `secret/<secret_name> created`.\n\nWe can get the list of secrets,\n\n```bash\nkubectl get secrets\n```\n"},{"type":"file","name":"09 Run Containers Inside k8s","children":null,"path":"../..//01 Onwards to k8s/09 Run Containers Inside k8s.md","content":"### Run Containers Inside k8s\n\n---\n\nNow we will run a `nginx` server in the `k8s cluster`. A single container of `nginx` can not be an objective for `k8s cluster`, but for simplicity and see how thinks get work, we are doing the demo.\n\nLoad the `Pod` config file,\n\n```bash\nkubectl apply -f client-pod.yml\n```\n\nWe should see the output `pod/proxy-pod created`.\n\nLoad the `Service` config file,\n\n```bash\nkubectl apply -f client-node-port.yml\n```\n\nWe should see the output `service/proxy-node-port created`.\n\nWe can get list of running pods,\n\n```bash\nkubectl get pods\n```\n\nWe should see `proxy-pod` in running status.\n\nWe can get the list of services,\n\n```bash\nkubectl get services\n```\n\nWe should see `proxy-node-port` in the service list.\n\nAll these services and ports are running not in the `localhost` instead, inside an IP provided by the `minikube`. We can see the provided IP by,\n\n```bash\nminikube ip\n```\n\nThis should give us an IP of the `k8s cluster`.\n\nNow, we can browse, `http://ip:31516` and see the `nginx` server is up and running.\n"},{"type":"file","name":"10 k8s Development Flow","children":null,"path":"../..//01 Onwards to k8s/10 k8s Development Flow.md","content":"## k8s Development Flow\n\n---\n\nIn the `k8s` world, we pass the config files like `Pod` or `Service` to the `master` using the `kubectl`. The `master` has a responsibility to determine how many container is being running inside the pods in various nodes.\n\nWe can define which nodes will run which container and how many. The master monitor all these containers inside the `Pod` of the `Node`. If some nodes fails to run or crash on runtime, the master will run another to keep the expectation.\n"},{"type":"file","name":"11 Imperative vs Declarative Deployments","children":null,"path":"../..//01 Onwards to k8s/11 Imperative vs Declarative Deployments.md","content":"### Imperative vs Declarative Deployments\n\n---\n\nWhenever we need to update/interact with a `k8s` cluster, we update/change the desired state in a config file and pass it to master. We should never directly go to the nodes directly and and update the nodes/pods/container.\n\nThe `master` is constantly working to meet the desired state.\n\nWith `Imperative Deployments`, we have to provide specific instructions step by step to reach a objective.\n\nBut when we go through the `Declarative Deployments`, we only define the final objective to the master and master will make it happen.\n\nWhenever it comes to update status it's always good practice to update the config file and feed it to the master and follow `Declarative Deployments`.\n"},{"type":"file","name":"12 Clear Environment","children":null,"path":"../..//01 Onwards to k8s/12 Clear Environment.md","content":"### Clear Environment\n\n---\n\nTo remove an object, we can take two approaches,\n\n1. Delete by object name, `kubectl delete object_type object_name`\n2. Delete by object file, `kubectl delete -f config_file_name`\n\nIf we want to delete by file name, we have to pass the right file name and path in as `config_file_name`.\n\nSome examples of clearing the `k8s` objects are,\n\n<u>**Removing Pods Object**</u> (Delete by file name)\n\nWe can remove an object from the `k8s` cluster by `kubectl delete -f config_file_name`. So the `Pod`, that is created from the config file `client-pod.yml` can be deleted by,\n\n```bash\nkubectl delete -f client-pod.yml\n```\n\nWe can verify `Pods` being deleted by get the list of the Pods,\n\n```bash\nkubectl get pods\n```\n\nThis is also an example of `Imperative Deployment`.\n\n<u>**Removing Deployments Object**</u> (Delete by object name)\n\nGet the list of deployments,\n\n```bash\nkubectl get deployments\n```\n\nThis should give us all the running `Deployment` object name.\n\nWe can delete the `Deployment` object by the name,\n\n```bash\nkubectl delete deployment deployment_object_name\n```\n\nAs output, we should see `deployment.apps \"deployment_object_name\" deleted`.\n\n<u>**Removing Services Object**</u> (Delete by object name)\n\nGet the list of services,\n\n```bash\nkubectl get services\n```\n\nThis should give us all the running `Service` object name.\n\nWe can delete the `Service` object by the name,\n\n```bash\nkubectl delete service service_object_name\n```\n\nAs output, we should see `service.apps \"service_object_name\" deleted`.\n\n> In the service list, there should be a service object named `kubernetes` and it is internally used by `kubernetes` itself. We should not delete or mess with this service.\n\nTo remove the whole cluster,\n\n```bash\nminikube delete\n```\n"}],"path":"../..//01 Onwards to k8s","content":""},{"type":"folder","name":"02 Maintain Contianers","children":[{"type":"file","name":"01 Update Existing Objects","children":null,"path":"../..//02 Maintain Contianers/01 Update Existing Objects.md","content":"### Update Existing Objects\n\n---\n\nWhen we need to update existing cluster state, we can either go imperative or declarative deployment.\n\nIf we are running a cluster with a `Pod` object and `Service` object. In declarative deployment, when we need to update the cluster, we have to make sure the `name` and `kind` of the object should be same. We will write the updated state in the config file. With `kubectl` we will feed the `master` updated config file, and the `master` will do the rest.\n\nTo do a hands on, first make sure we have a `Pod` and `Service`. The `Pod` config file `client-pod.yaml` is,\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: client-pod\n  labels:\n    component: web\nspec:\n  containers:\n    - name: client\n      image: stephengrider/multi-client\n      ports:\n        - containerPort: 3000\n```\n\nFeed this `Pod` object to the `master` by,\n\n```bash\nkubectl apply -f client-pod.yaml\n```\n\nAmd the `Service` config file `client-node-port.yaml` is,\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: client-node-port\nspec:\n  type: NodePort\n  ports:\n    - port: 3050\n      targetPort: 3000\n      nodePort: 31515\n  selector:\n    component: web\n```\n\nFeed this `Service` object to `master` by,\n\n```yaml\nkubectl apply -f client-node-port.yaml\n```\n"},{"type":"file","name":"02 Declarative Update","children":null,"path":"../..//02 Maintain Contianers/02 Declarative Update.md","content":"### Declarative Update\n\n---\n\nHere we take an existing config file, leave its `metadata->name` and `kind` same, but update the image from `stephengrider/multi-client` to `stephengrider/multi-worker`.\n\nIt's not our objective to run an entire application, we will just make sure, we can change the image of the pods.\n\nUpdated the `client-pod.yaml` we deployed earlier with new image `stephengrider/multi-worker`,\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: client-pod\n  labels:\n    component: web\nspec:\n  containers:\n    - name: client\n      image: stephengrider/multi-worker\n      ports:\n        - containerPort: 3000\n```\n\nNow feed the new `Pod` config file to master using `kubectl` by\n\n```bash\nkubectl apply -f client-pod.yaml\n```\n\nIn output, we should see `pod/client-pod configured`.\n\nWe can see all the pods list,\n\n```bash\nkubectl get pods\n```\n\nWe should see the `client-pod` in the list.\n\nWe can get details of the object info by, `kubectl describe <object type> <object name>`. In our case, we can look into `client-pod` details by,\n\n```bash\nkubectl describe Pod client-pod\n```\n\nIn the details output, under `Containers` we should see `Image: stephengrider/multi-worker`.\n\nThis is the exact image we used in the update config file. Also this can be considered as a declarative deployment.\n"},{"type":"file","name":"03 Limitations of Updating Config Files","children":null,"path":"../..//02 Maintain Contianers/03 Limitations of Updating Config Files.md","content":"### Limitations of Updating Config Files\n\n---\n\nWhen we update the config file for `Declarative Deployments` we can not update all the properties, only certain properties are there we are allowed to change like the `image`. For example, we can not update property like `containers`, `name`, `port` etc.\n"},{"type":"file","name":"04 Deployment Object Types","children":null,"path":"../..//02 Maintain Contianers/04 Deployment Object Types.md","content":"### Deployment Object Types\n\n---\n\nSince, we have some limitations of using `Pod` for `Declarative Deployment`, we can make use of another types of object `Deployment`. `Deployment` object is meant to maintain a set of identical Pods. It keep the states of the `Pod` updated and ensure number of `Pods`.\n\n**Pod Vs Deployment**\n\nBoth, `Pod` and `Deployment` are object types of `k8s`.\n\n<u>Pods</u>\n\n1. Run single set of tightly coupled containers\n2. Used in dev/local environment\n\n<u>Deployment</u>\n\n1. Runs set of identical `Pods`\n2. Monitor and keep the updated state of the `Pods`\n3. Good for dev and production environment\n\nWhen we create a `Deployment` object, we have to attach a `Pod Template`. A `Pod Template` is a block of `Pod` configuration. So essentially, every `Pod` we run with the `Deployment` object will follow the provided template configuration.\n\nAny time we change the config of the `Pods Template`, the `Deployment` first try to update the `Pod` configuration. If it fails to update `Pod` according to the updated `Pods Template` in this case, it will kill the existing `Pod` and create brand new `Pod` with updated template config.\n"},{"type":"file","name":"05 Deployment Config File","children":null,"path":"../..//02 Maintain Contianers/05 Deployment Config File.md","content":"### Deployment Config File\n\n---\n\nHere, we create a config file for the `Deployment` object and feed the config file using `kubectl` to the `master`.\n\nLike previous, we are going to use a `Pod Template` that is responsible to run the `multi-client` app.\n\nOur deployment file `client-deployment.yaml` should be as follows,\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: web\n  template:\n    metadata:\n      labels:\n        component: web\n    spec:\n      containers:\n        - name: client\n          image: stephengrider/multi-client\n          ports:\n            - containerPort: 3000\n```\n\n**apiVersion:** We are using the `Deployment` object, that is defined in the `apps/v1` api\n\n**Kind :** The object type is `Deployment`\n\n**metadata -> name :** This will be the created deployment object name `client-deployment`\n\n**spec -> selector :** With this selector, the `Deployment` object can handle the `Pod`. After a `Pod` is being create, the `Pod` also have a `select-by-labels` name. This should be same as the `spec -> selector` of the `Deployment` object.\n\n**spec -> replicas :** Number of pods will be created by the `Deployment Object`\n\n**spec -> template :** Config of the pods, will be used for every single pod we will create and maintain using the `Deployment Object`. This template is very much similar to the `Pod` config file.\n"},{"type":"file","name":"06 Apply Deployment","children":null,"path":"../..//02 Maintain Contianers/06 Apply Deployment.md","content":"### Apply Deployment\n\n---\n\nBefore we deploy the `client-deployment.yaml`, make sure existing `Pods` are being deleted. We can verify no Pods is being running by `kubectl get pods`.\n\nNow we can deploy and create a `Deployment Object` by,\n\n```bash\nkubectl apply -f client-deployment.yaml\n```\n\nWe should see an output `eployment.apps/client-deployment created`.\n\nIf we get the list of running pods,\n\n```bash\nkubectl get deployments\n```\n\nWe should see a `Deployment` object named `client-deployment`.\n"},{"type":"file","name":"07 Services Requirement","children":null,"path":"../..//02 Maintain Contianers/07 Services Requirement.md","content":"### Services Requirement\n\n---\n\nTo access the container from browser using localhost. The `minikube` creates a virtual machine with IP to access these containers.\n\nWe can get the ip of the virtual machine created by the `minikube`,\n\n```bash\nminikube ip\n```\n\nThis should return the IP of the virtual machine.\n\nIf we go to the `http://virtual_machine_op:31515/` from browser, we should see the react app.\n\n```bash\nkubectl get pods -o wide\n```\n\nThis will show the IP of the Pods. Every single Pods we have created always get an IP assigned. This IP address is internally assigned inside the Node and we can not directly access it from outside. If the Pod gets updated, changed or restarted, then the it is very much possible, the IP will be changed. So updating the IP in the development environment every time it is changed will be a big bottleneck. This is where we can make use of `Service`. With `Service`, the `Service` will be responsible to map traffic by the selector not the IP and we can get consistent address by the `Service` itself. So even though the Pods is being deleted, re-generated or restarted and getting a new IP, the `Service` object keeps these changes abstract to us.\n"},{"type":"file","name":"08 Updating Deployments","children":null,"path":"../..//02 Maintain Contianers/08 Updating Deployments.md","content":"### Updating Deployments\n\n---\n\nHere we update the deployments config file and verify the associated Pods are updating accordingly.\n\nTo do so, lets update the configuration file of the `client-deployment.yaml` with `containerPort: 9999`. The updated `client-deployment.yaml` should be,\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: web\n  template:\n    metadata:\n      labels:\n        component: web\n    spec:\n      containers:\n        - name: client\n          image: stephengrider/multi-client\n          ports:\n            - containerPort: 9999\n```\n\nNow, we feed the updated config file to the `master` by the `kubectl`,\n\n```bash\nkubectl apply -f client-deployment.yaml\n```\n\nWe should see output `deployment.apps/client-deployment configured`. This updated state essentially kill the existing Pod and create a new Pod with updated configuration.\n\nWe can see the list of deployments object,\n\n```bash\nkubectl get deployments\n```\n\nWe can see the updated/new Pod object by,\n\n```bash\nkubectl get pods\n```\n\nSince, with new configuration, we have to kill the exiting Pod and generate a new one, the pods age should be relatively very small.\n\nNow it is important to verify, the new Pod has the container port of `9999`. We can get pods details,\n\n```bash\nkubectl describe pods object_name_created_by_client_deployment_config_file\n```\n\nUnder `containers -> Client`, we should see `Port: 9999/TCP`. So without doubt, the Pod is running with up to date configuration.\n"},{"type":"file","name":"09 Scaling Deployments","children":null,"path":"../..//02 Maintain Contianers/09 Scaling Deployments.md","content":"### Scaling Deployments\n\n---\n\nIf we have a requirement to use `5 Pods` instead of `1 Pod`, we can update the deployment config files `replicas` to `5` by `replicas: 5`.\n\nOur scaled deployment config file, `client-deployment.yaml` should be following,\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client-deployment\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      component: web\n  template:\n    metadata:\n      labels:\n        component: web\n    spec:\n      containers:\n        - name: client\n          image: stephengrider/multi-client\n          ports:\n            - containerPort: 9999\n```\n\nNow feed the config file to `master` using `kubectl` by,\n\n```bash\nkubectl apply -f client-deployment.yaml\n```\n\nWe should get output `deployment.apps/client-deployment configured`.\n\n```bash\nkubectl get deployments\n```\n\nThis should listed a deployment object with `5/5`, i.e. 5 pods are desired and 5 pods are running.\n\nNow if we look for the pods list,\n\n```bash\nkubectl get pods\n```\n\nWe should see 5 Pods in running state, previously it was only 1 Pod.\n"},{"type":"file","name":"10 Updating Deployment Image","children":null,"path":"../..//02 Maintain Contianers/10 Updating Deployment Image.md","content":"### Updating Deployment Image\n\n---\n\nIn the world of `k8s` recreate the Pods for the updated version of image is quite complex. There's a popular [issue](https://github.com/kubernetes/kubernetes/issues/33664) in github on this topic. Traditionally we update/make change of the deployment file, we used to send the updated deployment file to the `master` using the `kubectl` command.\n\nIn the deployment config file, there is no particular version of the image. So In case the image is updated in the docker hub, but the config file is not changed. `kubectl` does not change any changes in config and simply reject the file.\n\n`k8s`, on our behalf does not go and check if a new version of the image is available or not.\n\nThis is why this is such a big issue.\n\nWith this in our mind, we can consider 3 possible issues,\n\n**Deleting Pod :** We can simply delete the Pods. In this case, for the missing Pod, the master will create the Pods to match the desired state. So this time hopefully the latest image will be pulled in. We should get Pods with latest version of images.\n\n**Image Tagging :** We can tag the images while building and also specify the version in the deployment config file.\n\n**Using Imperative Command :** In this case, we will put image tag every time we build a new image. But instead of putting the image tag in the deployment config file, we will use `imperative command` to update the image version.\n\nNone of these solutions are ideal. If we compare all these 3 options, we can definitely see the first one is very bad practice. Second one of `Image Tagging` is a pain for updating two config files every time. Among them `using imperative command` is comparatively much more usable.\n\nHere we will look into how can we update the deployment when a new version becomes available. We deploy the `deployment` with `stephengrider/multi-client` image, 3000 port and 1 replicas. Then we update the `multi-client` image and push the updated image to docker hub. Now ask the `Deployment Object` to recreate the Pods with the updated states.\n\nOur initial deployment config file `client-deployment.yaml` be like,\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: web\n  template:\n    metadata:\n      labels:\n        component: web\n    spec:\n      containers:\n        - name: client\n          image: stephengrider/multi-client\n          ports:\n            - containerPort: 3000\n```\n\nFeed this initial state to the `k8s cluster` by,\n\n```bash\nkubectl apply -f client-deployment.yaml\n```\n\nWe should see output `deployment.apps/client-deployment configured`.\n\nTo access the site, we first get the ip of the virtual machine,\n\n```bash\nminikube ip\n```\n\nThis should output the IP of the virtual machine.\n\nNow browse `http://virtual_machine_ip:31515/` we should see the react app is running.\n\nNow update the image. Let's say we are building new image with tag `v5`.\n\nWe can use `imperative command` to update the image inside the cluster using the format,\n\n`kubectl set image <object_type> / <object_name> <container_name> = <new_image>`\n\nIn our case, we can interpret as,\n\n```bash\nkubectl set image deployment/client-deployment client=stephengrider/multi-client:v5\n```\n\nNow in output, we should see `deployment.apps/client-deployment image updated`.\n\nWe can see the pods,\n\n```bash\nkubectl get pods\n```\n\nWe should notice the `AGE` of the pods is relatively small. This means without any doubt, the Pod is being recreated by the deployment.\n\nIf we now browse `http://virtual_machine_ip:31515/` we should see the react app is running and the title is changed to `Fib Calculator version 2`\n\nThis entire process is a little bit of pain.\n\n- We have to rebuild the image\n- Have to put a tag to the image\n- Run an `imperative` command to update the image\n\nIn production, there should be a script to handle these issues automatically.\n"},{"type":"file","name":"11 Multi Docker Access","children":null,"path":"../..//02 Maintain Contianers/11 Multi Docker Access.md","content":"### Multi Docker Access\n\n---\n\nWe have two docker installed\n\n1. Inside local machine\n2. Inside the virtual machine, created by minikube\n\nBy default our `docker-client` access the local `docker-server`. If we want to access the `docker-server` of the `virtual machine`, created by `minikube`, we can run,\n\n```bash\neval $(minikube docker-env)\n```\n\nFrom this terminal, we can access the `docker-server` of the virtual machine. This is not a permanent re-configuration.\n\n> This `eval $(minikube docker-env)` is a temporary connection of the virtual machine docker from the current terminal window. As soon as we go back to old terminal or open a new terminal, these terminal will make interaction of the local machine docker server.\n\nWhenever we run the `eval $(minikube docker-env)`, it actually exports couple of environment variable. We can see these environment variables by,\n\n```bash\nminikube docker-env\n```\n\nThis should export the variables, that helps the docker client to determine which server it is going to connect.\n\nThis command output also includes,\n\n```\n# To point your shell to minikube's docker-daemon, run:\n# eval $(minikube -p minikube docker-env)\n```\n\nThe second comment can guide us to configure the terminal to connect docker client with the virtual machine docker server.\n"},{"type":"file","name":"12 Requirement of Access Virtual Machine Docker Server","children":null,"path":"../..//02 Maintain Contianers/12 Requirement of Access Virtual Machine Docker Server.md","content":"### Requirement of Access Virtual Machine Docker Server\n\n---\n\nThere are couple of reasons, we might need to access the virtual machine docker server.\n\n- Do debugging, check logs, executing arbitrary program inside the containers etc\n- Testing k8s self healing by kill containers\n- Removed the cached images from the node\n\nAnd many more...\n"}],"path":"../..//02 Maintain Contianers","content":""},{"type":"folder","name":"03 Multi Container App","children":[{"type":"file","name":"- 01 Creating configurations","children":null,"path":"../..//03 Multi Container App/- 01 Creating configurations.md","content":"### Creating Configurations\n\n---\n\nTo create a secret for the postgres password,\n\n```bash\nkubectl create secret generic pgpassword --from-literal PGPASSWORD=1234asdf\n```\n"},{"type":"file","name":"- 02 Applying Configurations","children":null,"path":"../..//03 Multi Container App/- 02 Applying Configurations.md","content":"### Applying Configurations\n\n---\n\nWe are putting all the configurations in the `k8s` directory. With `kubectl`, instead of file, if we pass a directory as `apply` command, it will automatically look into the directory config files and apply them all on our behalf.\n\nBefore applying, make sure `minikube` is running by `minikube status`. If not, start the `minikube`\n\n```bash\nminikube start\n```\n\nTo apply all the config files inside `k8s`, we can run,\n\n```bash\nkubectl apply -f k8s\n```\n\nAs output, we should see the list of created/configured/unchanged object.\n\n---\n\nWe can check the deployment objects,\n\n```bash\nkubectl get deployments\n```\n\nThis should give us the output of deployment objects.\n\n---\n\nWe can check the pod objects,\n\n```bash\nkubectl get pods\n```\n\nThis should give us the output of pod objects.\n\nWe can take a look of the logs of container by,\n\n```bash\nkubectl logs pod_name\n```\n\n---\n\nWe can check the pod objects,\n\n```bash\nkubectl get pods\n```\n\nThis should give us the output of pod objects.\n"},{"type":"file","name":"01 App Architecture","children":null,"path":"../..//03 Multi Container App/01 App Architecture.md","content":"### App Architecture\n\n---\n"},{"type":"file","name":"02 Run Existing Application","children":null,"path":"../..//03 Multi Container App/02 Run Existing Application.md","content":"### Run Existing Application\n\n---\n"},{"type":"file","name":"03 Combining Config Files","children":null,"path":"../..//03 Multi Container App/03 Combining Config Files.md","content":"### Combining Config Files\n\n---\n\nThere's two ways we can organize the k8s config files in a directory.\n\n1. **Put multiple config files in a single file**: If we put multiple config files, we have to put a `---` between two config files. With this architecture, we can put close objects configuration together. But the downside is, an engineer have to actively understand the tightly coupled services for any changes.\n2. **Put each config file in individual file**: In this case, each object config will have an individual file with meaningful name. So any time, if there is a requirement to change/update the config it is easy to find. The downside is, there could be a lot of files of configuration.\n"},{"type":"file","name":"04 Data Persistant","children":null,"path":"../..//03 Multi Container App/04 Data Persistant.md","content":"### PVC\n\n---\n\nWhen we crete a `Deployment` object with `Postgres` image, we create a `Pod` inside the virtual machine. The `Postgres Container` inside the `Pod` has a storage system, which is only accessible by the container itself. While the application running, `Postgres` used to store data in its container file system. Anytime the `Pod` crashed, the container along with its file system will be lost. So, after crashing a `Pod` with `Postgres`, if another `Pod` with `Container` get created, the new container can not access or represent the old crashed container data/files.\n\n`PVC` stands for `persistent Volume Claims`. This `PVC` thing is very much similar to the docker worlds `volumes`. In docker, we used to persist volume by `Data Volume` and `Bind Mounting` to make sure using/mapping the host machine storage from the container.\n\nWe can come up architecture, where the `Postgres` will use the file system of the host machine. So in case the `Pod` crashed, and new `Pod` came up, the data in the host machine will not be lost. Also the new `Pods Container` can access these data.\n\n1. **Volume In K8s**: In docker world, we consider `Volume` as a file system inside the host machine, used/mapped by the container. But in `k8s` world, the `Volume` is an object, used to store data in the `Pod` level. In `k8s` when we create a `Volume`, we are creating a file system inside the `Pod`. The storage can be expressed as `Belong To` or `Associated To` the `Pods`. This `Volume` can be accessed by any container inside the `Pod`. Since `Volume` is tied with the `Pods`, if `Pod` dies, terminated or recreated, for any reason, the `Volume` itself will be gone as well. So, in `k8s`, the `Volume` is not appropriate for storing database.\n\n2. **Persistent Volume**: `Persistent Volume` is a long term durable storage, not tied to any `Pod` or `Container`. So, even a `Pod` or `Container` crashed, the data in the `Persisted Volume` survived.\n\n3. **Persistent Volume Claim**: `Persistent Volume Claim` is an advertizement of k8s for `Statically provisioned persistent volume` and `Dynamically provisioned persistent volume`. `Statically provisioned persistent volume` are created by k8s ahead of time but `Dynamically provisioned persistent volume` are created on demand on the fly. From `Pods config file` we can ask any of these and k8s will ensure these volumes on behalf of us.\n\n> A `Volume` is tied to Pod and can not survived if the pod delete/recreated/crashed. On the other hand, the `Persistent Volume` survived even though the `Pod` can not survive. The `Persistent Volume` only lost, if the administrator removed it.\n\n#### Diagram Of Volume: single volume inside pod with multiple container pointing it\n\n#### Diagram of Persistent Volume: a volume outside of the Pod, pointing multiple container\n"}],"path":"../..//03 Multi Container App","content":""},{"type":"folder","name":"k8s Deployments","children":[{"type":"file","name":"01 Deployment Process","children":null,"path":"../..//k8s Deployments/01 Deployment Process.md","content":""},{"type":"file","name":"02 GCP vs AWS for K8s","children":null,"path":"../..//k8s Deployments/02 GCP vs AWS for K8s.md","content":""}],"path":"../..//k8s Deployments","content":""}]